{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Frameworks for Machine Learning\n",
    "\n",
    "Machine Learning Fundamentals for Economists\n",
    "\n",
    "Jesse Perla (University of British Columbia)\n",
    "\n",
    "# Python\n",
    "\n",
    "## Why Python?\n",
    "\n",
    "-   For “modern” ML: **all** the well-supported frameworks are in Python\n",
    "-   In particular, auto-differentiation is central to many ML algorithms\n",
    "-   Why should you avoid Julia/Matlab/R in these cases?\n",
    "    -   Poor AD, especially for reverse-mode\n",
    "    -   Network effects. Very few higher level packages for ML pipeline\n",
    "    -   But Julia dominates for many ML topics (e.g. ODEs) and R is\n",
    "        outstanding for classic ML\n",
    "-   Should you use Python for more things?\n",
    "    -   Maybe, but it is limited and can be slow unless you jump through\n",
    "        hoops\n",
    "    -   Personally, if I have algorithms but no need for AD or\n",
    "        particular packages, Julia is a much better language and less\n",
    "        frustrating\n",
    "\n",
    "## There is No Such Thing as “Python”!\n",
    "\n",
    "-   Many incompatible wrappers around C++ for numerical methods\n",
    "-   Numpy/Scipy is the baseline (a common API)\n",
    "-   Pytorch\n",
    "-   JAX\n",
    "-   Ones to avoid\n",
    "    -   Tensorflow, common in industry but old\n",
    "    -   Numba (for me, reasonable people disagree)\n",
    "\n",
    "## Pytorch\n",
    "\n",
    "-   In recent years, the most flexible and popular ML framework for\n",
    "    researchers\n",
    "-   Key features:\n",
    "    -   Most of the code is for auto-differentiation/GPUs\n",
    "    -   JIT/etc. for GPU and fast kernels for deep learning\n",
    "    -   Neural Network libraries and utilities\n",
    "    -   A good subset of numpy\n",
    "    -   Utilities for ML pipelines optimization/etc.\n",
    "\n",
    "## Pytorch Key Downsides\n",
    "\n",
    "-   Not really for general purpose programming\n",
    "    -   Intended for making auto-differentiation of neural networks\n",
    "        easy, and updating gradients for solvers\n",
    "    -   May be very slow for simple things or ones which don’t involve\n",
    "        high-order AD\n",
    "-   Won’t always have packages you need for general code, and\n",
    "    compatibility is ugly\n",
    "\n",
    "## JAX\n",
    "\n",
    "-   Compiler that enables layered program transformations\n",
    "    1.  `jit` compiler to [XLA](https://www.tensorflow.org/xla/),\n",
    "        including accelerators (e.g. GPUs)\n",
    "    2.  `grad` Auto-differentiation\n",
    "    3.  `vmap` vectorization\n",
    "    4.  Flexibility to add more transformations\n",
    "-   [JAX\n",
    "    PyTrees](https://jax.readthedocs.io/en/latest/jax-101/05.1-pytrees.html)\n",
    "    provide a nested tree structure for compiler passes\n",
    "-   Closer to being a full JIT for general code than pytorch\n",
    "-   For ML, not full-featured like pytorch. Need to shop for other\n",
    "    libraries\n",
    "\n",
    "## JAX Key Downsides\n",
    "\n",
    "-   JAX is now stable and central to Google DeepMind’s infrastructure\n",
    "    -   Mature enough for production use, though API changes still occur\n",
    "-   Windows support has improved but Linux/macOS remain better supported\n",
    "-   Subset of python. Can’t really use loops, etc. Functional-style\n",
    "    programming\n",
    "    -   Much more restrictive than it seems, and far more restrictive\n",
    "        than pytorch\n",
    "\n",
    "# Python Ecosystem\n",
    "\n",
    "## Environments\n",
    "\n",
    "-   See [Python Environment Setup](../pages/python_setup.qmd) for\n",
    "    installation instructions and discussion of reproducibility\n",
    "-   `uv` is great as a `pip` replacement, but conda sometimes has better\n",
    "    binary support\n",
    "\n",
    "## Baseline, Safe Packages to Use\n",
    "\n",
    "-   [Numpy](https://numpy.org/doc/stable/) and\n",
    "    [Scipy](https://docs.scipy.org/doc/scipy/reference/)\n",
    "-   [Pandas](https://pandas.pydata.org/docs/) for dataframes\n",
    "-   [Matplotlib](https://matplotlib.org/stable/contents.html) for\n",
    "    general plotting\n",
    "-   [Seaborn](https://seaborn.pydata.org/) for plotting data\n",
    "-   [Statsmodels](https://www.statsmodels.org/stable/index.html) for\n",
    "    classic econometrics\n",
    "-   [Scikit-learn](https://scikit-learn.org/stable/) for classic ML\n",
    "\n",
    "## General Tools for ML Pipelines\n",
    "\n",
    "-   Logging/visualization: [Weights and Biases](https://wandb.ai/site)\n",
    "    -   Sign up for an account! Built in hyperparameter optimization\n",
    "        tools\n",
    "-   CLI useful for many pipelines and HPO. See\n",
    "    [here](https://github.com/shadawck/awesome-cli-frameworks#python)\n",
    "-   For more end-to-end frameworks for deep-learning\n",
    "    -   [Keras](https://keras.io/) is a higher-level framework for deep\n",
    "        learning. Traditionally tensorflow, but now many.\n",
    "    -   [Pytorch Lightning](https://lightning.ai) is easy and flexible,\n",
    "        eliminating a lot of boilerplate for CLI, optimizers, GPUs, etc.\n",
    "    -   Also [FastAI](https://www.fast.ai/)\n",
    "-   [HuggingFace](https://huggingface.co/) is a great resource for NLP\n",
    "    and transformers\n",
    "-   [Optuna](https://optuna.org/) is a great hyperparameter optimization\n",
    "    framework, etc.\n",
    "\n",
    "# JAX Ecosystem\n",
    "\n",
    "## Examples of Core Transformations\n",
    "\n",
    "From [JAX\n",
    "quickstart](https://jax.readthedocs.io/en/latest/notebooks/quickstart.html)\n",
    "\n",
    "Builtin composable transformations: `jit`, `grad` and `vmap`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "from jax import grad, jit, vmap, random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling with `jit`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1.45 ms ± 7.09 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n",
      "675 μs ± 9.5 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)"
     ]
    }
   ],
   "source": [
    "def selu(x, alpha=1.67, lmbda=1.05):\n",
    "  return lmbda * jnp.where(x > 0, x, alpha * jnp.exp(x) - alpha)\n",
    "key = random.key(0)  \n",
    "x = random.normal(key, (1000000,))\n",
    "%timeit selu(x).block_until_ready()\n",
    "selu_jit = jit(selu)\n",
    "%timeit selu_jit(x).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convenience Decorators for `jit`\n",
    "\n",
    "-   Convenience python decorator `@jit`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "670 μs ± 1.82 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)"
     ]
    }
   ],
   "source": [
    "@jit\n",
    "def selu(x, alpha=1.67, lmbda=1.05):\n",
    "  return lmbda * jnp.where(x > 0, x, alpha * jnp.exp(x) - alpha)\n",
    "%timeit selu(x).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Differentiation with `grad`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0.19661197 0.10499357 0.04517666]"
     ]
    }
   ],
   "source": [
    "def sum_logistic(x):\n",
    "  return jnp.sum(1.0 / (1.0 + jnp.exp(-x)))\n",
    "\n",
    "derivative_fn = grad(sum_logistic)\n",
    "x_small = jnp.array([1.0, 2.0, 3.0])\n",
    "print(derivative_fn(x_small))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual “Batching”/Vectorization\n",
    "\n",
    "Common to run the same function along one dimension of an array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "640 μs ± 2.98 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)"
     ]
    }
   ],
   "source": [
    "mat = random.normal(key, (150, 100))\n",
    "batched_x = random.normal(key, (10, 100))\n",
    "\n",
    "def f(v):\n",
    "  return jnp.dot(mat, v)\n",
    "def naively_batched_f(v_batched):\n",
    "  return jnp.stack([f(v) for v in v_batched])\n",
    "%timeit naively_batched_f(batched_x).block_until_ready()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using `vmap`\n",
    "\n",
    "The\n",
    "[vmap](https://jax.readthedocs.io/en/latest/_autosummary/jax.vmap.html#jax.vmap)\n",
    "applies across a dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Auto-vectorized with vmap\n",
      "32.8 μs ± 162 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)"
     ]
    }
   ],
   "source": [
    "@jit\n",
    "def vmap_batched_f(v_batched):\n",
    "  return vmap(f)(v_batched)\n",
    "\n",
    "print('Auto-vectorized with vmap')\n",
    "%timeit vmap_batched_f(batched_x).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More `vmap`\n",
    "\n",
    "Can fix dimensions with `in_axes`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "metadata": {},
     "data": {
      "text/plain": [
       "Array([ 0.,  3.,  6.,  9., 12.], dtype=float32)"
      ]
     }
    }
   ],
   "source": [
    "def f(a, x, y):\n",
    "  return a * x + y\n",
    "a = 2.0\n",
    "x = jnp.arange(5.)\n",
    "y = jnp.arange(5.)\n",
    "vmap(f, in_axes=(None, 0, 0))(a, x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save `vmap` functions\n",
    "\n",
    "Can fix dimensions with `in_axes`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "metadata": {},
     "data": {
      "text/plain": [
       "Array([ 0.,  3.,  6.,  9., 12.], dtype=float32)"
      ]
     }
    }
   ],
   "source": [
    "@jax.jit\n",
    "def f(a, x, y):\n",
    "  return a * x + y\n",
    "a = 2.0\n",
    "x = jnp.arange(5.)\n",
    "y = jnp.arange(5.)\n",
    "f_batched = vmap(f, in_axes=(None, 0, 0))\n",
    "f_batched(a, x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key JAX Neural Network Libraries/Frameworks\n",
    "\n",
    "-   Neural Network Libraries\n",
    "    -   [Flax NNX](https://flax.readthedocs.io/en/latest/index.html)\n",
    "        -   NNX is the new Flax API, Linen the older one\n",
    "        -   Has momentum, supported by google (for now)\n",
    "    -   [Equinox](https://github.com/patrick-kidger/equinox)\n",
    "        -   General, not just neural networks. Similar to NNX\n",
    "    -   [Keras](https://keras.io/) supports JAX (as well as PyTorch, TF,\n",
    "        etc.)\n",
    "\n",
    "## Other ML-oriented Packages\n",
    "\n",
    "-   Tough to keep up, see [Awesome\n",
    "    JAX](https://github.com/n2cholas/awesome-jax)\n",
    "-   [Optax](https://github.com/google-deepmind/optax) for ML-style\n",
    "    optimization\n",
    "-   Checkpointing and serialization:\n",
    "    [Orbax](https://orbax.readthedocs.io/en/latest/)\n",
    "\n",
    "## More Scientific Computing in JAX\n",
    "\n",
    "-   [jax.scipy](https://jax.readthedocs.io/en/latest/jax.scipy.html)\n",
    "    which is a subset of scipy\n",
    "-   Nonlinear Systems/Least Squares:\n",
    "    [Optimistix](https://github.com/patrick-kidger/optimistix)\n",
    "-   Linear Systems of Equations:\n",
    "    [Lineax](https://docs.kidger.site/lineax/)\n",
    "-   Matrix-free operators for iterative solvers:\n",
    "    [COLA](https://github.com/wilson-labs/cola)\n",
    "-   Differential Equations:\n",
    "    [diffrax](https://github.com/patrick-kidger/diffrax)\n",
    "-   More general optimization and solvers:\n",
    "    [JAXopt](https://jaxopt.github.io/stable/#)\n",
    "-   Interpolation:\n",
    "    [interpax](https://interpax.readthedocs.io/en/latest/?badge=latest)\n",
    "\n",
    "## JAX Challenges\n",
    "\n",
    "-   Basically only pure functional programming\n",
    "    -   No “mutation” of vectors\n",
    "    -   Loops/conditionals are tough\n",
    "    -   Rules for what is `jit`able are tricky\n",
    "-   See [JAX - The Sharp\n",
    "    Bits](https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html)\n",
    "-   May not be faster on CPUs or for “normal” things\n",
    "-   Debugging\n",
    "\n",
    "## PyTrees\n",
    "\n",
    "-   JAX uses a generic [tree\n",
    "    structure](https://jax.readthedocs.io/en/latest/jax-101/05.1-pytrees.html).\n",
    "    Powerful but takes time to understand: Examples from:\n",
    "    [here](https://jax.readthedocs.io/en/latest/pytrees.html) and\n",
    "    [here](https://jax.readthedocs.io/en/latest/_autosummary/jax.vmap.html#jax.vmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "11.0\n",
      "25.0\n",
      "[11. 25.]"
     ]
    }
   ],
   "source": [
    "f = lambda x, y: jnp.vdot(x, y)\n",
    "X = jnp.array([[1.0, 2.0],\n",
    "               [3.0, 4.0]])\n",
    "y = jnp.array([3.0, 4.0])\n",
    "print(f(X[0], y))\n",
    "print(f(X[1], y))\n",
    "\n",
    "mv = vmap(f, in_axes = (\n",
    "  0, # broadcast over 1st index of first argument\n",
    "  None # don't broadcast over anything of second argument\n",
    "  ), out_axes=0)\n",
    "print(mv(X, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTree Example 1\n",
    "\n",
    "The `in_axes` can match more complicated structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[1. 2. 3. 4. 5.]"
     ]
    }
   ],
   "source": [
    "dct = {'a': 0., 'b': jnp.arange(5.)}\n",
    "def foo(dct, x):\n",
    " return dct['a'] + dct['b'] + x\n",
    "# axes must match shape of the PyTree\n",
    "x = 1.\n",
    "out = vmap(foo, in_axes=(\n",
    "  {'a': None, 'b': 0}, #broadcast over the 'b'\n",
    "  None # no broadcasting over the \"x\"\n",
    "  ))(dct, x)\n",
    "# example now: {'a': 0, 'b': 0} etc.\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTree Example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[ 6. 10.]"
     ]
    }
   ],
   "source": [
    "dct = {'a': jnp.array([3.0, 5.0]), 'b': jnp.array([2.0, 4.0])}\n",
    "def foo2(dct, x):\n",
    " return dct['a'] + dct['b'] + x\n",
    "# axes must match shape of the PyTree\n",
    "x = 1.\n",
    "out = vmap(foo2, in_axes=(\n",
    "  {'a': 0, 'b': 0}, #broadcast over the 'a' and 'b'\n",
    "  None # no broadcasting over the \"x\"\n",
    "  ))(dct, x)\n",
    "# example now: {'a': 3.0, 'b': 2.0} etc.\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTree Example 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[16. 17. 18. 19. 20.]"
     ]
    }
   ],
   "source": [
    "dct = {'a': jnp.array([3.0, 5.0]), 'b': jnp.arange(5.)}\n",
    "def foo3(dct, x):\n",
    " return dct['a'][0] * dct['a'][1] + dct['b'] + x\n",
    "# axes must match shape of the PyTree\n",
    "out = vmap(foo3, in_axes=(\n",
    "  {'a': None, 'b': 0}, #broadcast over the 'b'\n",
    "  None # no broadcasting over the \"x\"\n",
    "  ))(dct, x)\n",
    "# example now: {'a': [3.0, 5.0], 'b': 0} etc.\n",
    "print(out)"
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "path": "/home/runner/work/grad_econ_ML/grad_econ_ML/.venv/share/jupyter/kernels/python3"
  },
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": "3"
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 }
}
