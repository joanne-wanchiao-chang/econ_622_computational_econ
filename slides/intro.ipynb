{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Course Overview and Computational Environment\n",
    "\n",
    "Graduate Economics and Machine Learning\n",
    "\n",
    "Jesse Perla (University of British Columbia)\n",
    "\n",
    "# Course Overview and Objectives\n",
    "\n",
    "## Course Overview\n",
    "\n",
    "-   First half of **ECON 622: Computational Economics with Data Science\n",
    "    Applications**\n",
    "-   This section will cover foundational theory and methods from machine\n",
    "    learning\n",
    "-   While we will cover applications, the emphasis will be on providing\n",
    "    enough mathematical foundations:\n",
    "    -   Understand these methods, and know their promises and\n",
    "        limitations\n",
    "    -   Adapt methods used in other disciplines to economic problems\n",
    "\n",
    "## Textbooks\n",
    "\n",
    "-   [Probabilistic Machine Learning: An\n",
    "    Introduction](https://probml.github.io/pml-book/book1.html) by Kevin\n",
    "    Murphy\n",
    "-   [Probabilistic Machine Learning: Advanced\n",
    "    Topics](https://probml.github.io/pml-book/book2.html) by Kevin\n",
    "    Murphy\n",
    "-   Both have PDFs of the “draft” available\n",
    "-   Code is available on https://github.com/probml/pyprobml\n",
    "    -   Much in JAX, some in torch/etc.\n",
    "-   For some linear algebra/etc. examples, see the [ECON526\n",
    "    Lectures](https://ubcecon.github.io/ECON526/lectures/lectures/index.html)\n",
    "\n",
    "# Statistical Learning and Functional Equations\n",
    "\n",
    "## Distributions\n",
    "\n",
    "-   bla\n",
    "\n",
    "## Risk Minimization\n",
    "\n",
    "-   bla\n",
    "\n",
    "## Empirical Risk Minimization (ERM)\n",
    "\n",
    "-   bla\n",
    "\n",
    "## Relevant Categories of Machine Learning\n",
    "\n",
    "-   Supervised Learning (e.g., Regression and Classification)\n",
    "-   Unsupervised Learning (e.g., Clustering, Auto-encoders)\n",
    "-   Semi-Supervised Learning (e.g., some observations)\n",
    "-   Reinforcement Learning (e.g., policy/control)\n",
    "-   Generative Models/Bayesian Methods (e.g., diffusions, probabilistic\n",
    "    programming)\n",
    "-   Instance-based learning (e.g., Kernel Methods) and Deep Learning are\n",
    "    somewhat orthogonal\n",
    "\n",
    "# Representations\n",
    "\n",
    "## “Depth” and Representations\n",
    "\n",
    "-   Could approximate a function $f(X)$ with a “shallow” approximation,\n",
    "    e.g. polynomials of $X$. Alternatively, nest functions $h(\\cdot)$\n",
    "    and $\\phi(\\cdot)$ $$\n",
    "    f(X) \\approx h(\\phi(X))\n",
    "    $$\n",
    "    -   First, the $\\phi(X)$ will transform the state into something\n",
    "        more amenable for the downstream task (e.g. prediction,\n",
    "        classification, etc.)\n",
    "    -   Then the $h(\\cdot)$ maps that transformed state into the output.\n",
    "-   Good $\\phi(\\cdot)$ efficiently calculate $h(\\cdot)$. Often reusable\n",
    "    for other tasks (e.g. $f_2(X) \\approx h_2(\\phi(X))$)\n",
    "-   For simple $X$ we can design them (e.g., take means, logs,\n",
    "    first-differences). But for rich data can we learn them from the\n",
    "    data itself?\n",
    "\n",
    "## Key Terminology: Features, Labels, and Latents\n",
    "\n",
    "-   **Features** are economists **explanatory or independent\n",
    "    variables**. They have the key source of variation to make\n",
    "    predictions and conduct counterfactuals\n",
    "-   **Labels** correspond to economists **observables or dependent\n",
    "    variables**\n",
    "-   **Latent Variables** are **unobserved variables**, typically sources\n",
    "    of heterogeneity or which may drive both the dependent and\n",
    "    independent variables\n",
    "-   **Feature Engineering** is the process of creating or selecting\n",
    "    features from the data that are more useful for the task at hand\n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "-   20th vs. 21st Century ML\n",
    "-   Stochastic Gradients and Auto-Differentiation\n",
    "-   Implicit and Explicit Regularization\n",
    "-   Inductive/Implicit Bias\n",
    "-   Generalization\n",
    "-   Overfitting and the Bias-Variance Tradeoff\n",
    "-   Test vs. Train vs. Validation Set\n",
    "-   Hyperparameter Optimization\n",
    "-   Representation Learning\n",
    "-   Transfer Learning\n",
    "\n",
    "# Optimization\n",
    "\n",
    "## Key\n",
    "\n",
    "-   TODO\n",
    "\n",
    "# Differentiation\n",
    "\n",
    "## Key\n",
    "\n",
    "-   TODO\n",
    "\n",
    "# Setup\n",
    "\n",
    "# Computational Environment\n",
    "\n",
    "## Programming Languages\n",
    "\n",
    "-   Python\n",
    "    -   “Raw” Numpy/skit-learn/etc.\n",
    "    -   Torch, JAX, etc.\n",
    "-   Julia\n",
    "-   In this half we will focus on Python, but Julia has advantages in\n",
    "    other areas.\n",
    "\n",
    "## Summary of Python Installation\n",
    "\n",
    "See [here](../pages/python_setup.qmd) for more details.\n",
    "\n",
    "1.  Install [git](https://git-scm.com/downloads)\n",
    "2.  Install [VS Code](https://code.visualstudio.com/)\n",
    "3.  Install [uv](https://github.com/astral-sh/uv) from terminal:\n",
    "    -   MacOS or Linux:\n",
    "        `curl -sSfL https://raw.githubusercontent.com/astral-sh/uv/main/install.sh | sh`\n",
    "    -   Windows:\n",
    "        `powershell -c \"irm https://astral.sh/uv/install.ps1 | more\"`\n",
    "        from powershell terminal\n",
    "\n",
    "## Clone Notebooks and Install Packages\n",
    "\n",
    "1.  Open the command palette with `<Ctrl+Shift+P>` or `<Cmd+Shift+P>` on\n",
    "    mac and type `> Git: Clone` and choose\n",
    "    `https://github.com/jlperla/grad_econ_ML_notebooks`\n",
    "2.  In VS Code terminal in that repo, `uv sync`\n",
    "3.  Then use VS Code to open any of the notebooks in that folder\n",
    "\n",
    "## Summary of Julia Installation\n",
    "\n",
    "See [here](../pages/julia_setup.qmd) for more details.\n",
    "\n",
    "1.  Install [Git](https://git-scm.com/install/)\n",
    "2.  Install [VS Code](https://code.visualstudio.com/)\n",
    "3.  Install Julia following the [Juliaup\n",
    "    instructions](https://github.com/JuliaLang/juliaup#installation)\n",
    "    -   Windows: `winget install julia -s msstore` in a terminal\n",
    "    -   Linux/Mac: `curl -fsSL https://install.julialang.org | sh` in a\n",
    "        terminal\n",
    "4.  Install the [VS Code Julia\n",
    "    extension](https://marketplace.visualstudio.com/items?itemName=julialang.language-julia)\n",
    "\n",
    "## Clone Notebooks and Install Packages\n",
    "\n",
    "1.  Open the command palette with `<Ctrl+Shift+P>` or `<Cmd+Shift+P>` on\n",
    "    mac and type `> Git: Clone` and choose\n",
    "    `https://github.com/jlperla/grad_econ_ML_notebooks`\n",
    "\n",
    "2.  Instantiate packages by running VS Code terminal\n",
    "\n",
    "    -   `] instantiate`, where `]` enters package mode\n",
    "\n",
    "3.  Then use VS Code to open any of the notebooks in that folder\n",
    "\n",
    "Note: the same clone’d repo can work for both Julia and Python"
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 }
}
