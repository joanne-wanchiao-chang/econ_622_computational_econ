{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More on Optimization using JAX\n",
    "\n",
    "Machine Learning Fundamentals for Economists\n",
    "\n",
    "Jesse Perla (University of British Columbia)\n",
    "\n",
    "# Linear Regression with Raw JAX\n",
    "\n",
    "## Packages\n",
    "\n",
    "-   `optax` is a common package for ML optimization methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import grad, jit, value_and_grad, vmap\n",
    "from jax import random\n",
    "import optax\n",
    "from flax import nnx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulate Data\n",
    "\n",
    "-   Few differences here, except for manual use of the `key`\n",
    "-   Remember that if you use the same `key` you get the same value.\n",
    "-   See [JAX\n",
    "    docs](https://jax.readthedocs.io/en/latest/jax-101/05-random-numbers.html)\n",
    "    for more details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 500  # samples\n",
    "M = 2\n",
    "sigma = 0.001\n",
    "key = random.PRNGKey(42)\n",
    "# Pattern: split before using key, replace name \"key\"\n",
    "key, *subkey = random.split(key, num=4)\n",
    "theta = random.normal(subkey[0], (M,))\n",
    "X = random.normal(subkey[1], (N, M))\n",
    "Y = X @ theta + sigma * random.normal(subkey[2], (N,))  # Adding noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloaders Provide Batches\n",
    "\n",
    "-   For more complicated data (e.g. images, text) JAX can use other\n",
    "    packages, but it doesn’t have a canonical dataloader at this point\n",
    "-   But in this case we can manually create this, using\n",
    "    [`yield`](https://docs.python.org/3/howto/functional.html#generators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "output-location": "column"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(Array([[-0.92034245, -0.7187076 ],\n",
      "       [-0.6151726 ,  0.47314   ],\n",
      "       [-0.35952824, -0.8299562 ],\n",
      "       [ 0.88198936, -0.3076048 ]], dtype=float32), Array([-1.1311196 ,  0.0050716 , -0.88230723,  0.28763232], dtype=float32))"
     ]
    }
   ],
   "source": [
    "def data_loader(key, X, Y, batch_size):\n",
    "    N = X.shape[0]\n",
    "    assert N == Y.shape[0]\n",
    "    indices = jnp.arange(N)\n",
    "    indices = random.permutation(key, indices)\n",
    "    # Loop over batches and yield\n",
    "    for i in range(0, N, batch_size):\n",
    "        b_indices = indices[i:i + batch_size]\n",
    "        yield X[b_indices], Y[b_indices]\n",
    "# e.g. iterate and get first element\n",
    "dl_test = data_loader(key, X, Y, 4)\n",
    "print(next(iter(dl_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis Class\n",
    "\n",
    "-   The “Hypothesis Class” for our ERM approximation is linear in this\n",
    "    case\n",
    "-   JAX is functional and non-mutating, so you must write stateless code\n",
    "-   We will move towards a more general class with the Flax NNX package,\n",
    "    but for now we will implement the model with the parameters directly\n",
    "-   The underlying parameters will have a random initialization, which\n",
    "    becomes **crucial** with overparameterized models (but wouldn’t be\n",
    "    important here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "theta_0 = [-0.21089035 -1.3627948 ], theta = [0.60576403 0.7990441 ]"
     ]
    }
   ],
   "source": [
    "def predict(theta, X):\n",
    "    return jnp.matmul(X, theta) #or jnp.dot(X, theta)\n",
    "\n",
    "# Need to randomize our own theta_0 parameters\n",
    "key, subkey = random.split(key)\n",
    "theta_0 = random.normal(subkey, (M,))\n",
    "print(f\"theta_0 = {theta_0}, theta = {theta}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function for Gradient Descent\n",
    "\n",
    "-   Reminder: need to provide AD-able functions which give a gradient\n",
    "    estimate, not necessarily the objective itself!\n",
    "-   In particular, for LLS we simply can find the MSE between the\n",
    "    prediction and the data for the batch itself\n",
    "-   For now, we are passing the `params` rather than the `model` itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorized_residuals(params, X, Y):\n",
    "    Y_hat = predict(params, X)\n",
    "    return jnp.mean((Y_hat - Y) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer\n",
    "\n",
    "-   The `optimizer.init(theta_0)` provides the initial state for the\n",
    "    iterations\n",
    "-   With SGD it is empty, but with momentum/etc. it will have internal\n",
    "    state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "output-location": "column"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Optimizer state:(EmptyState(), EmptyState())"
     ]
    }
   ],
   "source": [
    "lr = 0.001\n",
    "batch_size = 16\n",
    "num_epochs = 201\n",
    "\n",
    "# optax.adam(lr) is worse here\n",
    "optimizer = optax.sgd(lr)\n",
    "opt_state = optimizer.init(theta_0)\n",
    "print(f\"Optimizer state:{opt_state}\")\n",
    "params = theta_0 # initial condition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Optimizer for a Step\n",
    "\n",
    "-   Here we write a (compiled) utility function which:\n",
    "    1.  Calculates the loss and gradient estimates for the batch\n",
    "    2.  Updates the optimizer state\n",
    "    3.  Applies the updates to the parameters\n",
    "    4.  Returns the updated parameters, optimizer state, and loss\n",
    "-   The reason to set this up as a function is to maintain JAXs “pure”\n",
    "    style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def make_step(params, opt_state, X, Y):\n",
    "  loss_value, grads = jax.value_and_grad(vectorized_residuals)(params, X, Y)\n",
    "  updates, opt_state = optimizer.update(grads, opt_state, params)\n",
    "  params = optax.apply_updates(params, updates)\n",
    "  return params, opt_state, loss_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop Version 1\n",
    "\n",
    "-   Note that unlike Pytorch the gradients are passed as parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 0,||theta - theta_hat|| = 2.1659655570983887\n",
      "Epoch 100,||theta - theta_hat|| = 0.0036812787875533104\n",
      "Epoch 200,||theta - theta_hat|| = 6.539194873766974e-05\n",
      "||theta - theta_hat|| = 6.539194873766974e-05"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    key, subkey = random.split(key) # changing key for shuffling each epoch\n",
    "    train_loader = data_loader(subkey, X, Y, batch_size)\n",
    "    for X_batch, Y_batch in train_loader:\n",
    "        params, opt_state, train_loss = make_step(params, opt_state, X_batch, Y_batch)  \n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch},||theta - theta_hat|| = {jnp.linalg.norm(theta - params)}\")\n",
    "\n",
    "print(f\"||theta - theta_hat|| = {jnp.linalg.norm(theta - params)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto-Vectorizing\n",
    "\n",
    "-   In the above case the `vectorized_residuals` was able to use a\n",
    "    directly vectorized function.\n",
    "-   However in many cases it will be more convenient to write code for a\n",
    "    single element of the finite-sum objectives\n",
    "-   Now we will rewrite our objective to demonstrate how to use `vmap`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2.6319637\n",
      "5.4140573"
     ]
    }
   ],
   "source": [
    "def residual(theta, x, y):\n",
    "    y_hat = predict(theta, x)\n",
    "    return (y_hat - y) ** 2\n",
    "\n",
    "@jit\n",
    "def residuals(theta, X, Y):\n",
    "    # Use vmap, fixing the 1st argument\n",
    "    batched_residuals = jax.vmap(residual, in_axes=(None, 0, 0))\n",
    "    return jnp.mean(batched_residuals(theta, X, Y))\n",
    "print(residual(theta_0, X[0], Y[0]))\n",
    "print(residuals(theta_0, X, Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Step and Initialization\n",
    "\n",
    "-   This simply changes the function used for the `value_and_grad` call\n",
    "    to use the new `residuals` function and resets our optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def make_step(params, opt_state, X, Y):     \n",
    "  loss_value, grads = jax.value_and_grad(residuals)(params, X, Y)\n",
    "  updates, opt_state = optimizer.update(grads, opt_state, params)\n",
    "  params = optax.apply_updates(params, updates)\n",
    "  return params, opt_state, loss_value\n",
    "optimizer = optax.sgd(lr) # better than optax.adam here\n",
    "opt_state = optimizer.init(theta_0)\n",
    "params = theta_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop Version 2\n",
    "\n",
    "-   Otherwise the training loop is the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 0,||theta - theta_hat|| = 2.167938232421875\n",
      "Epoch 100,||theta - theta_hat|| = 0.003675078274682164\n",
      "Epoch 200,||theta - theta_hat|| = 6.522066541947424e-05\n",
      "||theta - theta_hat|| = 6.522066541947424e-05"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    key, subkey = random.split(key) # changing key for shuffling each epoch\n",
    "    train_loader = data_loader(subkey, X, Y, batch_size)\n",
    "    for X_batch, Y_batch in train_loader:\n",
    "        params, opt_state, train_loss = make_step(params, opt_state, X_batch, Y_batch)  \n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch},||theta - theta_hat|| = {jnp.linalg.norm(theta - params)}\")\n",
    "\n",
    "print(f\"||theta - theta_hat|| = {jnp.linalg.norm(theta - params)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JAX Examples\n",
    "\n",
    "-   See\n",
    "    [examples/linear_regression_jax_sgd.py](examples/linear_regression_jax_sgd.py)\n",
    "    -   This implements the inline code above without the vmap\n",
    "-   See\n",
    "    [examples/linear_regression_jax_vmap.py](examples/linear_regression_jax_vmap.py)\n",
    "    -   This implements the `vmap` as above\n",
    "    -   This also adds in an [learning rate\n",
    "        schedule](https://optax.readthedocs.io/en/latest/api.html#optimizer-schedules)\n",
    "-   See\n",
    "    [examples/linear_regression_jax_nnx.py](examples/linear_regression_jax_nnx.py)\n",
    "    and\n",
    "    [examples/linear_regression_jax_nnx_split.py](examples/linear_regression_jax_nnx_split.py)\n",
    "    for ones using the Flax NNX\n",
    "\n",
    "# Linear Regression with Flax\n",
    "\n",
    "## Flax NNX\n",
    "\n",
    "-   While it seems convenient to work in a functional style, when we\n",
    "    move towards nested, deep approximations it can become cumbersome to\n",
    "    manage the parameters\n",
    "-   [Flax](https://flax.readthedocs.io/en/latest/index.html) is a\n",
    "    package which provides flexible ways to define and work with\n",
    "    function approximations\n",
    "    -   There is a newer (NNX) and older (Linen) interface. Use NNX.\n",
    "-   We will also introduce a DataLoader class to remove boilerplate\n",
    "\n",
    "## Hypothesis Class\n",
    "\n",
    "-   We are moving towards Neural Networks, which are a very broad class\n",
    "    of approximations.\n",
    "-   Here lets just use a linear approximation with no constant term\n",
    "-   As always, the initial randomization will become increasingly\n",
    "    important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "N, M, sigma = 500, 2, 0.001\n",
    "rngs = nnx.Rngs(42)\n",
    "model = nnx.Linear(M, 1, use_bias=False, rngs=rngs)\n",
    "print(model.kernel) # the initial parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Residuals Using the “Model”\n",
    "\n",
    "-   The model now contains all of the, potentially nested, parameters\n",
    "    for the approximation class\n",
    "-   It provides call notation to evaluate the function with those\n",
    "    parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual(model, x, y):\n",
    "    y_hat = model(x)\n",
    "    return (y_hat - y) ** 2\n",
    "\n",
    "def residuals_loss(model, X, Y):\n",
    "    return jnp.mean(jax.vmap(residual, in_axes=(None, 0, 0))(model, X, Y))\n",
    "theta = random.normal(rngs(), (M,))\n",
    "X = random.normal(rngs(), (N, M))\n",
    "Y = X @ theta + sigma * random.normal(rngs(), (N,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradients of Models\n",
    "\n",
    "-   As discussed, we can find the gradients of richer objects than just\n",
    "    arrays\n",
    "-   Optimizer updates use perturbations of the underlying PyTree\n",
    "-   Updates can be applied because the type of the gradients matches the\n",
    "    underlying PyTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "output-location": "column"
   },
   "outputs": [],
   "source": [
    "grads = nnx.grad(residuals_loss)(model, X, Y)\n",
    "print(grads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Optimizer and Training Step\n",
    "\n",
    "-   Note the `@nnx.jit` which replaces `@jax.jit`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "@nnx.jit\n",
    "def train_step(model, optimizer, X, Y):\n",
    "    def loss_fn(model):\n",
    "        return residuals_loss(model, X, Y)\n",
    "    loss, grads = nnx.value_and_grad(loss_fn)(model)\n",
    "    optimizer.update(model, grads)\n",
    "    return loss\n",
    "optimizer = nnx.Optimizer(model, optax.sgd(0.001), wrt=nnx.Param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Optimizer\n",
    "\n",
    "-   Run optimizer and extract the parameters in the `model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/tmp/ipykernel_6206/1113100642.py:9: DeprecationWarning: '.value' access is now deprecated. For Variable[Array] instances use:\n",
      "\n",
      "  variable[...]\n",
      "\n",
      "For other Variable types use:\n",
      "\n",
      "  variable.get_value()\n",
      "\n",
      "  norm_diff = jnp.linalg.norm(theta - jnp.squeeze(model.kernel.value))"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 0,||theta-theta_hat|| = 1.2717349529266357"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/tmp/ipykernel_6206/1113100642.py:9: DeprecationWarning: '.value' access is now deprecated. For Variable[Array] instances use:\n",
      "\n",
      "  variable[...]\n",
      "\n",
      "For other Variable types use:\n",
      "\n",
      "  variable.get_value()\n",
      "\n",
      "  norm_diff = jnp.linalg.norm(theta - jnp.squeeze(model.kernel.value))"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 100,||theta-theta_hat|| = 0.24903634190559387"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/tmp/ipykernel_6206/1113100642.py:9: DeprecationWarning: '.value' access is now deprecated. For Variable[Array] instances use:\n",
      "\n",
      "  variable[...]\n",
      "\n",
      "For other Variable types use:\n",
      "\n",
      "  variable.get_value()\n",
      "\n",
      "  norm_diff = jnp.linalg.norm(theta - jnp.squeeze(model.kernel.value))"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 200,||theta-theta_hat|| = 0.04919437691569328"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/tmp/ipykernel_6206/1113100642.py:9: DeprecationWarning: '.value' access is now deprecated. For Variable[Array] instances use:\n",
      "\n",
      "  variable[...]\n",
      "\n",
      "For other Variable types use:\n",
      "\n",
      "  variable.get_value()\n",
      "\n",
      "  norm_diff = jnp.linalg.norm(theta - jnp.squeeze(model.kernel.value))"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 300,||theta-theta_hat|| = 0.00985759124159813"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/tmp/ipykernel_6206/1113100642.py:9: DeprecationWarning: '.value' access is now deprecated. For Variable[Array] instances use:\n",
      "\n",
      "  variable[...]\n",
      "\n",
      "For other Variable types use:\n",
      "\n",
      "  variable.get_value()\n",
      "\n",
      "  norm_diff = jnp.linalg.norm(theta - jnp.squeeze(model.kernel.value))"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 400,||theta-theta_hat|| = 0.002040109597146511\n",
      "||theta - theta_hat|| = 0.0004721158475149423"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/tmp/ipykernel_6206/1113100642.py:11: DeprecationWarning: '.value' access is now deprecated. For Variable[Array] instances use:\n",
      "\n",
      "  variable[...]\n",
      "\n",
      "For other Variable types use:\n",
      "\n",
      "  variable.get_value()\n",
      "\n",
      "  norm_diff = jnp.linalg.norm(theta - jnp.squeeze(model.kernel.value))"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "for epoch in range(500):\n",
    "    key, subkey = random.split(key)\n",
    "    train_loader = data_loader(subkey, X, Y, batch_size)\n",
    "    for X_batch, Y_batch in train_loader:\n",
    "        loss = train_step(model, optimizer, X_batch, Y_batch)\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        norm_diff = jnp.linalg.norm(theta - jnp.squeeze(model.kernel.value))\n",
    "        print(f\"Epoch {epoch},||theta-theta_hat|| = {norm_diff}\")\n",
    "norm_diff = jnp.linalg.norm(theta - jnp.squeeze(model.kernel.value))\n",
    "print(f\"||theta - theta_hat|| = {norm_diff}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a Custom Type\n",
    "\n",
    "-   “Neural Networks” are custom types which nest parameterized function\n",
    "    calls\n",
    "-   Nest calls to other `nnx.Module` or create/use differentiable\n",
    "    `nnx.Param`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLinear(nnx.Module):\n",
    "    def __init__(self, in_size, out_size, rngs):\n",
    "        self.out_size = out_size\n",
    "        self.in_size = in_size\n",
    "        self.kernel = nnx.Param(jax.random.normal(rngs(), (self.out_size, self.in_size)))\n",
    "    # Similar to Pytorch's forward\n",
    "    def __call__(self, x):\n",
    "        return self.kernel @ x\n",
    "\n",
    "model = MyLinear(M, 1, rngs = rngs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Same Optimization Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/tmp/ipykernel_6206/1919407154.py:7: DeprecationWarning: '.value' access is now deprecated. For Variable[Array] instances use:\n",
      "\n",
      "  variable[...]\n",
      "\n",
      "For other Variable types use:\n",
      "\n",
      "  variable.get_value()\n",
      "\n",
      "  norm_diff = jnp.linalg.norm(theta - jnp.squeeze(model.kernel.value))"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 0,||theta-theta_hat|| = 0.6275200247764587\n",
      "Epoch 100,||theta-theta_hat|| = 0.6275200247764587\n",
      "Epoch 200,||theta-theta_hat|| = 0.6275200247764587\n",
      "Epoch 300,||theta-theta_hat|| = 0.6275200247764587\n",
      "Epoch 400,||theta-theta_hat|| = 0.6275200247764587\n",
      "||theta - theta_hat|| = 0.6275200247764587"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/tmp/ipykernel_6206/1919407154.py:9: DeprecationWarning: '.value' access is now deprecated. For Variable[Array] instances use:\n",
      "\n",
      "  variable[...]\n",
      "\n",
      "For other Variable types use:\n",
      "\n",
      "  variable.get_value()\n",
      "\n",
      "  norm_diff = jnp.linalg.norm(theta - jnp.squeeze(model.kernel.value))"
     ]
    }
   ],
   "source": [
    "optimizer = nnx.Optimizer(model, optax.sgd(0.001), wrt=nnx.Param)\n",
    "for epoch in range(500):\n",
    "    for X_batch, Y_batch in train_loader:\n",
    "        loss = train_step(model, optimizer, X_batch, Y_batch)\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        norm_diff = jnp.linalg.norm(theta - jnp.squeeze(model.kernel.value))\n",
    "        print(f\"Epoch {epoch},||theta-theta_hat|| = {norm_diff}\")\n",
    "norm_diff = jnp.linalg.norm(theta - jnp.squeeze(model.kernel.value))\n",
    "print(f\"||theta - theta_hat|| = {norm_diff}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering Transformations\n",
    "\n",
    "-   Much of the NNX package is built around\n",
    "    [filtering](https://flax.readthedocs.io/en/latest/guides/filters_guide.html)\n",
    "    members of the underlying python class\n",
    "-   Within an `nnx.Module` the `nnx.Param` are values which you might\n",
    "    look to differentiate, others are fixed\n",
    "-   Since JAX code is (primarily) “pure” and functional, a key part of\n",
    "    the package is to split and recombine parameters intended for\n",
    "    gradients from those which are not\n",
    "\n",
    "## Splitting into Differentiable Parameters\n",
    "\n",
    "-   For our custom type, the fields are `out_size, in_size, kernel`. We\n",
    "    only want to differentate the `kernel` since wrapped in `nnx.Param`\n",
    "-   To separate out parameters use `nnx.split` and to recombine use\n",
    "    `nnx.merge`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "output-location": "column"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "GraphDef(nodes=[NodeDef(\n",
      "  type='MyLinear',\n",
      "  index=0,\n",
      "  outer_index=None,\n",
      "  num_attributes=5,\n",
      "  metadata=MyLinear\n",
      "), NodeDef(\n",
      "  type='GenericPytree',\n",
      "  index=None,\n",
      "  outer_index=None,\n",
      "  num_attributes=0,\n",
      "  metadata=({}, PyTreeDef(CustomNode(PytreeState[(False, False)], [])))\n",
      "), VariableDef(\n",
      "  type='Param',\n",
      "  index=1,\n",
      "  outer_index=None,\n",
      "  metadata=PrettyMapping({\n",
      "    'is_hijax': False,\n",
      "    'has_ref': False,\n",
      "    'is_mutable': True,\n",
      "    'eager_sharding': True\n",
      "  })\n",
      ")], attributes=[('_pytree__nodes', Static(value={'_pytree__state': True, 'out_size': False, 'in_size': False, 'kernel': True, '_pytree__nodes': False})), ('_pytree__state', NodeAttr()), ('in_size', Static(value=2)), ('kernel', NodeAttr()), ('out_size', Static(value=1))], num_leaves=1)"
     ]
    }
   ],
   "source": [
    "model = MyLinear(M, 1, rngs = rngs)\n",
    "graphdef, state = nnx.split(model)\n",
    "print(graphdef)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging\n",
    "\n",
    "-   `graphdef` was the fixed structure, `state` is the differentiable\n",
    "-   Use `nnx.merge` to combine the fixed and differentiable parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "output-location": "column"
   },
   "outputs": [],
   "source": [
    "print(state)\n",
    "# Emulate a \"gradient\" update\n",
    "def apply_fake_gradient(param):\n",
    "    return param + 0.01\n",
    "# Apply \"gradient\" update to tree\n",
    "state_2 = jax.tree_util.tree_map(\n",
    "               apply_fake_gradient, state)\n",
    "# Combine to form a model\n",
    "model_2 = nnx.merge(graphdef, state_2)\n",
    "print(model_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Advanced Optimization Loops\n",
    "\n",
    "-   Filtering is often automated by replacing `jax` with `nnx`\n",
    "    equivalents\n",
    "    -   `nnx.jit, nnx.value_and_grad` etc. automatically filter for\n",
    "        Params\n",
    "-   This process provides some overhead, so for high-speed\n",
    "    [examples](https://github.com/google/flax/blob/main/examples/nnx_toy_examples/01_functional_api.py)\n",
    "    may manually split and merge"
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "path": "/home/runner/work/grad_econ_ML/grad_econ_ML/.venv/share/jupyter/kernels/python3"
  },
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": "3"
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 }
}
