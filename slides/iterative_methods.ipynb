{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numerical Linear Algebra with Iterative Methods\n",
    "\n",
    "Machine Learning Fundamentals for Economists\n",
    "\n",
    "Jesse Perla (University of British Columbia)\n",
    "\n",
    "# Overview\n",
    "\n",
    "## Motivation\n",
    "\n",
    "-   Building on the [previous\n",
    "    lecture’s](factorizations_direct_methods.qmd) direct methods, we now\n",
    "    explore iterative approaches\n",
    "\n",
    "-   Iterative methods and matrix conditioning you’ll learn:\n",
    "\n",
    "    -   **Conditioning**: Why some matrices are harder to work with\n",
    "        (condition numbers)\n",
    "    -   **Stationary methods**: Jacobi iteration for diagonally dominant\n",
    "        systems\n",
    "    -   **Krylov methods**: Conjugate Gradient (CG) and NormalCG for\n",
    "        least squares\n",
    "    -   **Matrix-free operators**: Solving problems without storing the\n",
    "        full matrix\n",
    "    -   **Preconditioning**: Transforming problems to make them easier\n",
    "        to solve\n",
    "    -   **Applications**: Large-scale LLS, two-way fixed effects, CTMC\n",
    "        value functions\n",
    "\n",
    "-   Key insight: Performance depends on geometry (conditioning), not\n",
    "    just size\n",
    "\n",
    "-   These methods are essential for ML: optimization, regularization,\n",
    "    and large-scale problems\n",
    "\n",
    "## Summary and Material\n",
    "\n",
    "-   See [QuantEcon Krylov Methods and Matrix\n",
    "    Conditioning](https://julia.quantecon.org/tools_and_techniques/iterative_methods_sparsity.html)\n",
    "-   Python resources:\n",
    "    -   [Lineax Documentation](https://docs.kidger.site/lineax/) - JAX\n",
    "        linear solvers \\[@lineax2023\\]\n",
    "    -   [CoLA Documentation](https://cola.readthedocs.io/) -\n",
    "        Compositional Linear Algebra \\[@potapczynski2023cola\\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.random as random\n",
    "import lineax as lx\n",
    "import cola\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "key = random.PRNGKey(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conditioning\n",
    "\n",
    "## Direct Methods and Conditioning\n",
    "\n",
    "-   Some algorithms and some matrices are more numerically stable than\n",
    "    others\n",
    "    -   By “numerically stable” we mean sensitive to accumulated\n",
    "        roundoff errors\n",
    "-   A key issue is when matrices are close to singular, or almost have\n",
    "    collinear columns. Many times this can’t be avoided, other times it\n",
    "    can (e.g., choose orthogonal polynomials rather than monomials)\n",
    "-   This will become even more of an issue with iterative methods, but\n",
    "    is also the key to rapid convergence. Hint: $A x = b$ is easy if\n",
    "    $A = I$, even if it is dense.\n",
    "\n",
    "## Condition Numbers of Matrices\n",
    "\n",
    "-   $\\det(A) \\approx 0$ may say it is “almost” singular, but it is not\n",
    "    scale-invariant\n",
    "-   The condition number $\\kappa$, given matrix norm $||\\cdot||$ uses\n",
    "    the matrix norm\n",
    "\n",
    "$$\n",
    "\\text{cond}(A) \\equiv \\|A\\| \\|A^{-1}\\|\\geq 1\n",
    "$$\n",
    "\n",
    "-   Expensive to calculate, can show that given spectrum\n",
    "\n",
    "$$\n",
    "\\text{cond}(A) = \\left|\\frac{\\lambda_{max}}{\\lambda_{min}}\\right|\n",
    "$$\n",
    "\n",
    "-   Intuition: if $\\text{cond}(A) = K$, then $b \\to b + \\nabla b$ change\n",
    "    in $b$ amplifies to a $x \\to x + K \\nabla b$ error when solving\n",
    "    $A x = b$.\n",
    "-   See [Matlab Docs on\n",
    "    inv](https://www.mathworks.com/help/matlab/ref/inv.html#bu6sfy8-1)\n",
    "    for why `inv` is a bad idea when $\\text{cond}(A)$ is huge\n",
    "\n",
    "## Condition Numbers and Matrix Operations\n",
    "\n",
    "-   The identity matrix is as good as it gets\n",
    "-   Otherwise, the issue is when matrices are of fundamentally different\n",
    "    scales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "output-location": "column"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "cond(A2) = 2.00e+06\n",
      "cond(A2.T) = 2.00e+06\n",
      "cond(inv(A2)) = 1.88e+06"
     ]
    }
   ],
   "source": [
    "epsilon = 1E-6\n",
    "A2 = jnp.array([[1.0, 0.0],\n",
    "                [1.0, epsilon]])\n",
    "print(f\"cond(A2) = {jnp.linalg.cond(A2):.2e}\")\n",
    "print(f\"cond(A2.T) = {jnp.linalg.cond(A2.T):.2e}\")\n",
    "print(f\"cond(inv(A2)) = {jnp.linalg.cond(jnp.linalg.inv(A2)):.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditioning Under Matrix Products\n",
    "\n",
    "-   Matrix operations can often amplify the condition number, or may be\n",
    "    invariant\n",
    "-   Be especially careful with normal equations/etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "output-location": "column"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "cond(L) = 1.73e+08\n",
      "cond(L.T @ L) = inf\n",
      "Matrix L:\n",
      "[[1.e+00 1.e+00 1.e+00]\n",
      " [1.e-08 0.e+00 0.e+00]\n",
      " [0.e+00 1.e-08 0.e+00]\n",
      " [0.e+00 0.e+00 1.e-08]]"
     ]
    }
   ],
   "source": [
    "def lauchli(N, epsilon):\n",
    "    \"\"\"Construct Lauchli matrix\"\"\"\n",
    "    ones_row = jnp.ones((1, N))\n",
    "    eye_scaled = epsilon * jnp.eye(N)\n",
    "    return jnp.vstack([ones_row, eye_scaled])\n",
    "\n",
    "epsilon = 1E-8\n",
    "L = lauchli(3, epsilon)\n",
    "print(f\"cond(L) = {jnp.linalg.cond(L):.2e}\")\n",
    "print(f\"cond(L.T @ L) = {jnp.linalg.cond(L.T @ L):.2e}\")\n",
    "print(\"Matrix L:\")\n",
    "print(L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See\n",
    "[here](https://julia.quantecon.org/tools_and_techniques/iterative_methods_sparsity.html#why-a-monomial-basis-is-a-bad-idea)\n",
    "for why a monomial basis is a bad idea\n",
    "\n",
    "# Stationary Iterative Methods\n",
    "\n",
    "## Direct Methods\n",
    "\n",
    "-   Direct methods work with a matrix, stored in memory, and typically\n",
    "    involve factorizations\n",
    "    -   Can be dense or sparse\n",
    "    -   They can be fast, and solve problems to machine precision\n",
    "-   Typically are superior until problems get large or have particular\n",
    "    structure\n",
    "-   But always use the right factorizations and matrix structure! (e.g.,\n",
    "    posdef, sparse, etc)\n",
    "-   The key limitations are the sizes of the matrices (or the sparsity)\n",
    "\n",
    "## Iterative Methods\n",
    "\n",
    "-   Iterative methods are in the spirit of gradient descent and\n",
    "    optimization algorithms\n",
    "    -   They take an initial guess and update until convergence\n",
    "    -   They work on matrix-vector and vector-matrix products, and can\n",
    "        be **matrix-free**, which is a huge advantage for huge problems\n",
    "    -   Rather than waiting until completion like direct methods, you\n",
    "        can control stopping\n",
    "-   The key limitations on performance are geometric (e.g.,\n",
    "    conditioning), not dimensionality\n",
    "-   Two rough types: stationary methods and Krylov methods\n",
    "\n",
    "## Bellman Equation with CTMC Generator\n",
    "\n",
    "-   Let $r \\in \\mathbb{R}^N$ be a vector of payoffs in each state, and\n",
    "    $\\rho > 0$ a discount rate\n",
    "-   Then we can use the $Q$ generator as a simple Bellman Equation\n",
    "    (using the Kolmogorov Backwards Equation) to find the value $v$ in\n",
    "    each state\n",
    "\n",
    "$$\n",
    "\\rho v = r + Q v\n",
    "$$\n",
    "\n",
    "-   Rearranging, $(\\rho I - Q) v = r$\n",
    "-   Teaser: can we just implement $(\\rho I - Q)\\cdot v$ and avoid\n",
    "    factorizing the matrix?\n",
    "\n",
    "## Example from Previous Lectures\n",
    "\n",
    "-   Variation on CTMC example: $a >0$ gain, $b > 0$ to lose\n",
    "-   Solve the Bellman Equation for a CTMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mean value: 101.963066"
     ]
    }
   ],
   "source": [
    "N = 100\n",
    "a = 0.1\n",
    "b = 0.05\n",
    "rho = 0.05\n",
    "\n",
    "# Define diagonals for tridiagonal matrix Q\n",
    "lower_diag = jnp.full(N-1, b)\n",
    "main_diag = jnp.concatenate([jnp.array([-a]),\n",
    "                             jnp.full(N-2, -(a+b)),\n",
    "                             jnp.array([-b])])\n",
    "upper_diag = jnp.full(N-1, a)\n",
    "Q = cola.ops.Tridiagonal(lower_diag, main_diag, upper_diag)\n",
    "\n",
    "# For direct solve, convert to dense\n",
    "r = jnp.linspace(0.0, 10.0, N)\n",
    "Q_dense = Q.to_dense()\n",
    "A = rho * jnp.eye(N) - Q_dense\n",
    "v_direct = jnp.linalg.solve(A, r)\n",
    "print(f\"Mean value: {jnp.mean(v_direct):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagonal Dominance\n",
    "\n",
    "-   Stationary Iterative Methods reorganize the problem so it is a\n",
    "    contraction mapping and then iterate\n",
    "-   For matrices that are [**strictly diagonal\n",
    "    dominant**](https://en.wikipedia.org/wiki/Diagonally_dominant_matrix)\n",
    "\n",
    "$$\n",
    "|A_{ii}| \\geq \\sum_{j\\neq i} |A_{ij}| \\quad\\text{for all } i = 1\\ldots N\n",
    "$$\n",
    "\n",
    "-   i.e., sum of all off-diagonal elements in a row is less than the\n",
    "    diagonal element in absolute value\n",
    "\n",
    "-   Note for our problem rows sum to 0 so if $\\rho > 0$ then\n",
    "    $\\rho I - Q$ is strictly diagonally dominant\n",
    "\n",
    "## Jacobi Iteration\n",
    "\n",
    "-   To solve a system $A x = b$, split the matrix $A$ into its diagonal\n",
    "    and off-diagonal elements. That is,\n",
    "\n",
    "$$\n",
    "A \\equiv D + R\n",
    "$$\n",
    "\n",
    "$$\n",
    "D \\equiv \\begin{bmatrix} A_{11} & 0 & \\ldots & 0\\\\\n",
    "                    0    & A_{22} & \\ldots & 0\\\\\n",
    "                    \\vdots & \\vdots & \\vdots & \\vdots\\\\\n",
    "                    0 & 0 &  \\ldots & A_{NN}\n",
    "    \\end{bmatrix}\\,\\,\n",
    "R \\equiv \\begin{bmatrix} 0 & A_{12}  & \\ldots & A_{1N} \\\\\n",
    "                    A_{21}    & 0 & \\ldots & A_{2N} \\\\\n",
    "                    \\vdots & \\vdots & \\vdots & \\vdots\\\\\n",
    "                    A_{N1}  & A_{N2}  &  \\ldots & 0\n",
    "    \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "## Jacobi Iteration Algorithm\n",
    "\n",
    "-   Then we can rewrite $(D + R) x = b$ as\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "D x &= b - R x\\\\\n",
    "x &= D^{-1} (b - R x)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Where $D^{-1}$ is trivial since diagonal. To solve, take an iteration\n",
    "$x^k$, starting from $x^0$,\n",
    "\n",
    "$$\n",
    "x^{k+1} = D^{-1}(b - R x^k)\n",
    "$$\n",
    "\n",
    "See [<span class=\"button\">Jacobi\n",
    "Implementation</span>](#sec-jacobi-appendix) in appendix for code\n",
    "example.\n",
    "\n",
    "# Krylov Methods\n",
    "\n",
    "## Krylov Subspaces\n",
    "\n",
    "-   Krylov methods are a class of iterative methods that use a sequence\n",
    "    of subspaces\n",
    "-   The subspaces are generated by repeated matrix-vector products\n",
    "    -   i.e., given an $A$ and a initial value $b$ we could generate the\n",
    "        sequence\n",
    "    -   $b, A b, A^2 b, \\ldots, A^k b$ and see\n",
    "-   Note that the only operation we require from our linear operator $A$\n",
    "    is the matrix-vector product. This is a huge advantage for large\n",
    "    problems\n",
    "-   e.g. Krylov method is [Conjugate\n",
    "    Gradient](https://en.wikipedia.org/wiki/Conjugate_gradient_method)\n",
    "    for posdef $A$\n",
    "\n",
    "## Structured and Lazy Operators\n",
    "\n",
    "-   **CoLA** (Compositional Linear Algebra) \\[@potapczynski2023cola\\]\n",
    "    provides structured matrix types\n",
    "-   Operators can be composed lazily without materializing the result\n",
    "-   **Example**: Diagonal + Tridiagonal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Three smallest eigenvalues: [0.89475226+0.j 1.8502488 +0.j 2.850001  +0.j]"
     ]
    }
   ],
   "source": [
    "# Diagonal operator\n",
    "D = cola.ops.Diagonal(jnp.arange(1.0, N+1))\n",
    "\n",
    "# Compose without forming the matrix (lazy composition)\n",
    "Op = Q + D\n",
    "\n",
    "# CoLA handles the structure automatically in solves\n",
    "b_test = jnp.ones(N)\n",
    "x_cola = cola.solve(Op, b_test)\n",
    "\n",
    "# Can also use eigenvalue methods\n",
    "eigenvalues, eigenvectors = cola.eig(Op, k=3, which='SM')\n",
    "print(f\"Three smallest eigenvalues: {eigenvalues}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benefits of Lazy Composition\n",
    "\n",
    "-   No need to materialize `Q + D` as a dense matrix\n",
    "-   CoLA dispatches to appropriate algorithms based on structure\n",
    "-   Memory efficient for large problems\n",
    "-   Enables matrix-free methods at scale\n",
    "\n",
    "## Conjugate Gradient\n",
    "\n",
    "-   CG method for positive-definite systems, matrix or function form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "cond(A) = 6.32e+01, Iterations: 32, Error: 2.61e-05"
     ]
    }
   ],
   "source": [
    "N_cg = 100\n",
    "key, subkey = random.split(key)\n",
    "A_sparse = random.uniform(subkey, (N_cg, N_cg))\n",
    "key, subkey = random.split(key)\n",
    "A_sparse = jnp.where(random.uniform(subkey, (N_cg, N_cg)) < 0.1, A_sparse, 0.0)\n",
    "A_pd = A_sparse @ A_sparse.T + 0.5 * jnp.eye(N_cg)\n",
    "key, subkey = random.split(key)\n",
    "b_cg = random.uniform(subkey, (N_cg,))\n",
    "x_direct = jnp.linalg.solve(A_pd, b_cg)\n",
    "operator = lx.MatrixLinearOperator(A_pd, tags=lx.positive_semidefinite_tag)\n",
    "solver = lx.CG(rtol=1e-5, atol=1e-5, max_steps=1000)\n",
    "solution = lx.linear_solve(operator, b_cg, solver)\n",
    "print(f\"cond(A) = {jnp.linalg.cond(A_pd):.2e}, Iterations: {solution.stats['num_steps']}, Error: {jnp.linalg.norm(solution.value - x_direct):.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarking: CG vs Direct Solve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Direct solve: 1.03 ms\n",
      "CG solve: 0.48 ms\n",
      "Speedup: 2.13x"
     ]
    }
   ],
   "source": [
    "# Direct solve benchmark\n",
    "start = time.perf_counter()\n",
    "x_direct = jnp.linalg.solve(A_pd, b_cg)\n",
    "x_direct.block_until_ready()  # Wait for JAX async execution\n",
    "direct_time = time.perf_counter() - start\n",
    "\n",
    "# CG solve benchmark\n",
    "start = time.perf_counter()\n",
    "solution = lx.linear_solve(operator, b_cg, solver)\n",
    "solution.value.block_until_ready()\n",
    "cg_time = time.perf_counter() - start\n",
    "\n",
    "print(f\"Direct solve: {direct_time*1000:.2f} ms\")\n",
    "print(f\"CG solve: {cg_time*1000:.2f} ms\")\n",
    "print(f\"Speedup: {direct_time/cg_time:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key insights**:\n",
    "\n",
    "-   Iterative methods scale better for large sparse systems\n",
    "-   Direct methods may be faster for small/dense matrices\n",
    "-   Conditioning affects iteration count\n",
    "\n",
    "## Iterative Methods for LLS\n",
    "\n",
    "-   [NormalCG](https://docs.kidger.site/lineax/api/solvers/#lineax.NormalCG)\n",
    "    \\[@lineax2023\\] is a Krylov method for solving least squares via the\n",
    "    normal equations\n",
    "\n",
    "$$\n",
    "\\min_{\\beta} \\| X \\beta -y \\|^2 + \\alpha  \\| \\beta\\|^2\n",
    "$$\n",
    "\n",
    "-   Where $\\alpha \\geq 0$. If $\\alpha = 0$ then it delivers the\n",
    "    ridgeless regression limit, even if underdetermined\n",
    "\n",
    "## NormalCG Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Norm difference: 1.97e-04, Iterations: 15"
     ]
    }
   ],
   "source": [
    "M = 1000\n",
    "N_lls = 10000\n",
    "sigma = 0.1\n",
    "key, subkey = random.split(key)\n",
    "X_sparse = random.uniform(subkey, (N_lls, M))\n",
    "key, subkey = random.split(key)\n",
    "X_sparse = jnp.where(random.uniform(subkey, (N_lls, M)) < 0.1, X_sparse, 0.0)\n",
    "key, subkey = random.split(key)\n",
    "beta_true = random.uniform(subkey, (M,))\n",
    "key, subkey = random.split(key)\n",
    "y = X_sparse @ beta_true + sigma * random.normal(subkey, (N_lls,))\n",
    "beta_direct = jnp.linalg.lstsq(X_sparse, y, rcond=None)[0]\n",
    "operator = lx.MatrixLinearOperator(X_sparse)\n",
    "solver = lx.NormalCG(rtol=1e-5, atol=1e-5, max_steps=1000)\n",
    "solution = lx.linear_solve(operator, y, solver)\n",
    "beta_normalcg = solution.value\n",
    "print(f\"Norm difference: {jnp.linalg.norm(beta_direct - beta_normalcg):.2e}, Iterations: {solution.stats['num_steps']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarking: Direct vs Iterative LLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Direct lstsq: 1244.58 ms\n",
      "NormalCG: 134.40 ms\n",
      "Speedup: 9.26x\n",
      "Iterations: 15"
     ]
    }
   ],
   "source": [
    "# Benchmark direct least squares\n",
    "start = time.perf_counter()\n",
    "beta_direct = jnp.linalg.lstsq(X_sparse, y, rcond=None)[0]\n",
    "beta_direct.block_until_ready()\n",
    "direct_time = time.perf_counter() - start\n",
    "\n",
    "# Benchmark NormalCG\n",
    "start = time.perf_counter()\n",
    "solution = lx.linear_solve(operator, y, solver)\n",
    "solution.value.block_until_ready()\n",
    "normalcg_time = time.perf_counter() - start\n",
    "\n",
    "print(f\"Direct lstsq: {direct_time*1000:.2f} ms\")\n",
    "print(f\"NormalCG: {normalcg_time*1000:.2f} ms\")\n",
    "print(f\"Speedup: {direct_time/normalcg_time:.2f}x\")\n",
    "print(f\"Iterations: {solution.stats['num_steps']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Trade-offs**:\n",
    "\n",
    "-   Overdetermined systems (N \\> M): iterative methods shine\n",
    "-   Sparse matrices: memory savings matter\n",
    "-   Accuracy: iterative methods controlled by tolerances\n",
    "\n",
    "## Matrix-Free LLS\n",
    "\n",
    "-   For LLS, need $X u$ and $X^T v$ products via\n",
    "    `FunctionLinearOperator`\n",
    "-   Lineax automatically computes transposes (no manual adjoints\n",
    "    needed!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Norm diff: 7.22e-04, Iterations: 16"
     ]
    }
   ],
   "source": [
    "def matvec(vec):\n",
    "    return X_sparse @ vec\n",
    "input_structure = jax.ShapeDtypeStruct((M,), jnp.float32)\n",
    "X_op = lx.FunctionLinearOperator(matvec, input_structure)\n",
    "solver = lx.NormalCG(rtol=1e-5, atol=1e-5, max_steps=1000)\n",
    "solution = lx.linear_solve(X_op, y, solver)\n",
    "beta_matvec = solution.value\n",
    "print(f\"Norm diff: {jnp.linalg.norm(beta_direct - beta_matvec):.2e}, Iterations: {solution.stats['num_steps']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eigenvalue Problems\n",
    "\n",
    "## Eigenvalue Example\n",
    "\n",
    "-   Steady state of CTMC is solution to $Q^{\\top} \\cdot \\bar{\\pi} = 0$\n",
    "-   The $\\bar{\\pi}$ left-eigenvector associated with eigenvalue 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "λ_min: -2.50e-01, Mean(φ): 0.250000, Q.T:\n",
      "[[-0.1   0.05  0.    0.  ]\n",
      " [ 0.1  -0.15  0.05  0.  ]\n",
      " [ 0.    0.1  -0.15  0.05]\n",
      " [ 0.    0.    0.1  -0.05]]"
     ]
    }
   ],
   "source": [
    "N_eig = 4\n",
    "a = 0.1\n",
    "b = 0.05\n",
    "lower_diag = jnp.full(N_eig-1, b)\n",
    "main_diag = jnp.concatenate([jnp.array([-a]), jnp.full(N_eig-2, -(a+b)), jnp.array([-b])])\n",
    "upper_diag = jnp.full(N_eig-1, a)\n",
    "Q_eig = cola.ops.Tridiagonal(lower_diag, main_diag, upper_diag)\n",
    "Q_T = cola.ops.Tridiagonal(upper_diag, main_diag, lower_diag)\n",
    "eigenvalues, eigenvectors = cola.eig(Q_T, k=1, which='SM')\n",
    "lambda_min = eigenvalues[0].real\n",
    "phi = eigenvectors[:, 0].real\n",
    "phi = phi / jnp.sum(phi)\n",
    "print(f\"λ_min: {lambda_min:.2e}, Mean(φ): {jnp.mean(phi):.6f}, Q.T:\\n{Q_T.to_dense()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Matrix-Free Operator for Adjoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Matrix-free error: 3.73e-09"
     ]
    }
   ],
   "source": [
    "def Q_adj_product(x):\n",
    "    first = -a * x[0] + b * x[1]\n",
    "    middle = a * x[:-2] - (a + b) * x[1:-1] + b * x[2:]\n",
    "    last = a * x[-2] - b * x[-1]\n",
    "    return jnp.concatenate([jnp.array([first]), middle, jnp.array([last])])\n",
    "\n",
    "key, subkey = random.split(key)\n",
    "x_check = random.uniform(subkey, (N_eig,))\n",
    "Q_dense = Q_eig.to_dense()\n",
    "error = jnp.linalg.norm(Q_adj_product(x_check) - Q_dense.T @ x_check)\n",
    "print(f\"Matrix-free error: {error:.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving with Matrix-Free Operator\n",
    "\n",
    "-   The `FunctionLinearOperator` wrapper adds features required for\n",
    "    algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Smallest eigenvalue (matrix-free): -2.50e-01\n",
      "Mean of eigenvector: 0.250000"
     ]
    }
   ],
   "source": [
    "# Wrap in Lineax FunctionLinearOperator\n",
    "input_structure = jax.ShapeDtypeStruct((N_eig,), jnp.float32)\n",
    "Q_adj_op = lx.FunctionLinearOperator(Q_adj_product, input_structure)\n",
    "\n",
    "# For eigenvalues, we can use CoLA with the dense version\n",
    "# (CoLA's matrix-free operator support is limited for eigenvalue problems)\n",
    "Q_dense_T = Q_dense.T\n",
    "Q_cola = cola.ops.Dense(Q_dense_T)\n",
    "\n",
    "# Find smallest eigenvalue\n",
    "eigenvalues_mf, eigenvectors_mf = cola.eig(Q_cola, k=1, which='SM')\n",
    "lambda_min_mf = eigenvalues_mf[0].real\n",
    "phi_mf = eigenvectors_mf[:, 0].real\n",
    "phi_mf = phi_mf / jnp.sum(phi_mf)\n",
    "\n",
    "print(f\"Smallest eigenvalue (matrix-free): {lambda_min_mf:.2e}\")\n",
    "print(f\"Mean of eigenvector: {jnp.mean(phi_mf):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preconditioning\n",
    "\n",
    "## Changing the Geometry\n",
    "\n",
    "-   In practice, most Krylov methods are preconditioned or else direct\n",
    "    methods usually dominate. Same with large nonlinear systems\n",
    "-   As discussed, the key issue for the convergence speed of iterative\n",
    "    methods is the geometry (e.g. condition number of hessian, etc)\n",
    "-   Preconditioning changes the geometry. e.g. more like circles or with\n",
    "    eigenvalue problems spread out the eigenvalues of interest\n",
    "-   Preconditioners for a matrix $A$ requires art and tradeoffs\n",
    "    -   Want be relatively cheap to calculate, and must be invertible\n",
    "    -   Want to have $\\text{cond}(P A) \\ll \\text{cond}(A)$\n",
    "-   Ideal preconditioner for $A x = b$ is $P=A^{-1}$ since\n",
    "    $A^{-1} A x = x = A^{-1} b$\n",
    "    -   $\\text{cond}(A^{-1}A)=1$! But that is equivalent to solving\n",
    "        problem\n",
    "\n",
    "## Right-Preconditioning a Linear System\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "A x &= b\\\\\n",
    "A P^{-1} P x &= b\\\\\n",
    "A P^{-1} y &= b\\\\\n",
    "P x &= y\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "That is, solve $(A P^{-1})y = b$ for $y$, and then solve $P x = y$ for\n",
    "$x$.\n",
    "\n",
    "## Raw Conjugate Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "cond(A) = 2.26e+02, Iterations: 59"
     ]
    }
   ],
   "source": [
    "N_precond = 200\n",
    "key, subkey = random.split(key)\n",
    "A_sparse_precond = random.uniform(subkey, (N_precond, N_precond))\n",
    "key, subkey = random.split(key)\n",
    "A_sparse_precond = jnp.where(random.uniform(subkey, (N_precond, N_precond)) < 0.1, A_sparse_precond, 0.0)\n",
    "A_precond = A_sparse_precond @ A_sparse_precond.T + 0.5 * jnp.eye(N_precond)\n",
    "key, subkey = random.split(key)\n",
    "b_precond = random.uniform(subkey, (N_precond,))\n",
    "operator_precond = lx.MatrixLinearOperator(A_precond, tags=lx.positive_semidefinite_tag)\n",
    "solver_precond = lx.CG(rtol=1e-6, atol=1e-6, max_steps=1000)\n",
    "solution_no_precond = lx.linear_solve(operator_precond, b_precond, solver_precond)\n",
    "print(f\"cond(A) = {jnp.linalg.cond(A_precond):.2e}, Iterations: {solution_no_precond.stats['num_steps']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagonal Preconditioner\n",
    "\n",
    "-   A simple preconditioner is the diagonal of $A$\n",
    "-   Cheap to calculate, invertible if diagonal has no zeros\n",
    "-   We precondition by solving\n",
    "    $D^{-1/2} A D^{-1/2} (D^{1/2} x) = D^{-1/2} b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_inv_sqrt = 1.0 / jnp.sqrt(jnp.diag(A_precond))\n",
    "P_diag = jnp.diag(D_inv_sqrt)\n",
    "A_precond_system = P_diag @ A_precond @ P_diag\n",
    "b_precond_system = P_diag @ b_precond\n",
    "operator_precond_system = lx.MatrixLinearOperator(A_precond_system, tags=lx.positive_semidefinite_tag)\n",
    "solution_precond = lx.linear_solve(operator_precond_system, b_precond_system, solver_precond)\n",
    "x_precond = P_diag @ solution_precond.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagonal Preconditioner Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Iterations (with diagonal preconditioner): 57\n",
      "Reduction: 3.4%\n",
      "Error: 1.55e-05"
     ]
    }
   ],
   "source": [
    "print(f\"Iterations (with diagonal preconditioner): {solution_precond.stats['num_steps']}\")\n",
    "print(f\"Reduction: {(1 - solution_precond.stats['num_steps']/solution_no_precond.stats['num_steps'])*100:.1f}%\")\n",
    "print(f\"Error: {jnp.linalg.norm(A_precond @ x_precond - b_precond):.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarking: Preconditioning Impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Without preconditioner: 59 iterations\n",
      "With diagonal precond: 57 iterations\n",
      "Reduction: 3.4%\n",
      "\n",
      "Condition numbers:\n",
      "cond(A): 2.26e+02\n",
      "cond(P A P): 2.26e+02"
     ]
    }
   ],
   "source": [
    "print(f\"Without preconditioner: {solution_no_precond.stats['num_steps']} iterations\")\n",
    "print(f\"With diagonal precond: {solution_precond.stats['num_steps']} iterations\")\n",
    "reduction_pct = (1 - solution_precond.stats['num_steps']/solution_no_precond.stats['num_steps'])*100\n",
    "print(f\"Reduction: {reduction_pct:.1f}%\")\n",
    "print(f\"\\nCondition numbers:\")\n",
    "print(f\"cond(A): {jnp.linalg.cond(A_precond):.2e}\")\n",
    "print(f\"cond(P A P): {jnp.linalg.cond(A_precond_system):.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**When preconditioning helps**:\n",
    "\n",
    "-   Poorly conditioned systems (high κ)\n",
    "-   Multiple solves with same operator\n",
    "-   Complex problem structure\n",
    "-   Diagonal preconditioning reduces condition number\n",
    "\n",
    "## Incomplete Factorizations in JAX\n",
    "\n",
    "-   **Limitation**: Incomplete LU/Cholesky preconditioners are not\n",
    "    available in the JAX ecosystem\n",
    "-   JAX’s sparse matrix support is still experimental\n",
    "    (`jax.experimental.sparse`)\n",
    "-   No mature libraries for ILU preconditioners exist for JAX (as of\n",
    "    2026)\n",
    "\n",
    "**Alternatives**: - Diagonal preconditioners (available in Lineax) -\n",
    "Algebraic multigrid methods (not covered here) - Interface with external\n",
    "libraries (SciPy, PETSc) via callbacks for production use - Julia’s\n",
    "`IncompleteLU.jl` for comparison\n",
    "\n",
    "**Sources**: [JAX GitHub Discussion\n",
    "#18452](https://github.com/jax-ml/jax/discussions/18452), [JAX Sparse\n",
    "Documentation](https://docs.jax.dev/en/latest/jax.experimental.sparse.html)\n",
    "\n",
    "## Others\n",
    "\n",
    "-   In the above we aren’t getting huge gains, but it is also lacking\n",
    "    structure\n",
    "-   If you have problems with multiple scales, as might come out of\n",
    "    discretizing multiple dimensions in a statespace, see\n",
    "    [multigrid](https://en.wikipedia.org/wiki/Multigrid_method) methods\n",
    "    -   Algebraic Multigrid preconditioner is often useful even outside\n",
    "        of having different scales\n",
    "-   Other preconditioners include ones intended for [Graph\n",
    "    Laplacians](https://github.com/danspielman/Laplacians.jl) such as\n",
    "    approximate cholesky decompositions and combinatorial multigrid\n",
    "    preconditioners.\n",
    "    -   See [paper](https://arxiv.org/abs/2303.00709) for more\n",
    "\n",
    "# Appendices\n",
    "\n",
    "## Jacobi Iteration (Educational) [<span class=\"button\">Back</span>](#sec-jacobi-ref)\n",
    "\n",
    "SOR (Successive Over-Relaxation) is more complex to implement\n",
    "functionally in JAX due to the need for sequential updates. Use Krylov\n",
    "methods instead for better performance. - Educational implementation of\n",
    "Jacobi iteration using `jax.lax.scan`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Error after 40 iterations: 1.77e-03"
     ]
    }
   ],
   "source": [
    "# Use the CTMC example from earlier\n",
    "A_jacobi = A  # From the CTMC example\n",
    "b_jacobi = r\n",
    "v_direct_jacobi = v_direct\n",
    "\n",
    "# Jacobi iteration: x^{k+1} = D^{-1}(b - R x^k)\n",
    "def jacobi_step(x, _):\n",
    "    \"\"\"Single Jacobi iteration step (functional, no mutation)\"\"\"\n",
    "    D_inv = 1.0 / jnp.diag(A_jacobi)\n",
    "    R = A_jacobi - jnp.diag(jnp.diag(A_jacobi))\n",
    "    x_new = D_inv * (b_jacobi - R @ x)\n",
    "    return x_new, None\n",
    "\n",
    "# Run 40 iterations using scan\n",
    "x0 = jnp.zeros(N)\n",
    "x_jacobi, _ = jax.lax.scan(jacobi_step, x0, None, length=40)\n",
    "error_jacobi = jnp.linalg.norm(x_jacobi - v_direct_jacobi, ord=jnp.inf)\n",
    "print(f\"Error after 40 iterations: {error_jacobi:.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "path": "/home/runner/work/grad_econ_ML/grad_econ_ML/.venv/share/jupyter/kernels/python3"
  },
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": "3"
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 }
}
