{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Direct Methods and Matrix Factorizations\n",
    "\n",
    "Machine Learning Fundamentals for Economists\n",
    "\n",
    "Jesse Perla (University of British Columbia)\n",
    "\n",
    "# Overview\n",
    "\n",
    "## Motivation\n",
    "\n",
    "-   In preparation for the ML lectures we cover core numerical linear\n",
    "    algebra concepts\n",
    "\n",
    "-   Direct methods and matrix factorizations you’ll learn:\n",
    "\n",
    "    -   **Computational complexity**: Big-O notation and understanding\n",
    "        what makes operations expensive\n",
    "    -   **Matrix structure**: Exploiting sparsity, triangular,\n",
    "        tridiagonal, and positive-definite structure\n",
    "    -   **Factorizations**: LU, Cholesky, and eigenvalue decompositions\n",
    "        for solving linear systems\n",
    "    -   **Applications**: Continuous Time Markov Chains and Bellman\n",
    "        equations\n",
    "\n",
    "-   These methods solve problems to machine precision but scale with\n",
    "    matrix size\n",
    "\n",
    "-   In the [next lecture](iterative_methods.qmd), we’ll see iterative\n",
    "    methods that trade precision for scalability\n",
    "\n",
    "## Packages and Materials\n",
    "\n",
    "-   See [QuantEcon Numerical Linear\n",
    "    Algebra](https://julia.quantecon.org/tools_and_techniques/numerical_linear_algebra.html)\n",
    "    and associated notebooks\n",
    "-   Python resources:\n",
    "    -   [JAX SciPy\n",
    "        Documentation](https://jax.readthedocs.io/en/latest/jax.scipy.html) -\n",
    "        JAX implementations of SciPy functions\n",
    "    -   [Lineax Documentation](https://docs.kidger.site/lineax/) -\n",
    "        Linear solvers \\[@lineax2023\\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.scipy.linalg as jla\n",
    "import lineax as lx\n",
    "import scipy.sparse as sp\n",
    "import scipy.sparse.linalg as spla\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "key = jax.random.PRNGKey(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complexity\n",
    "\n",
    "## Basic Computational Complexity\n",
    "\n",
    "**Big-O Notation**\n",
    "\n",
    "For a function $f(N)$ and a positive constant $C$, we say $f(N)$ is\n",
    "$O(g(N))$, if there exist positive constants $C$ and $N_0$ such that:\n",
    "\n",
    "$$\n",
    "0 \\leq f(N) \\leq C \\cdot g(N) \\quad \\text{for all } N \\geq N_0\n",
    "$$\n",
    "\n",
    "-   Often crucial to know how problems scale asymptotically (as\n",
    "    $N\\to\\infty$)\n",
    "-   Caution! This is only an asymptotic limit, and can be misleading for\n",
    "    small $N$\n",
    "    -   $f_1(N) = N^3 + N$ is $O(N^3)$\n",
    "    -   $f_2(N) = 1000 N^2 + 3 N$ is $O(N^2)$\n",
    "    -   For roughly $N>1000$ use $f_2$ algorithm, otherwise $f_1$\n",
    "\n",
    "## Examples of Computational Complexity\n",
    "\n",
    "-   Simple examples:\n",
    "    -   $x \\cdot y = \\sum_{n=1}^N x_n y_n$ is $O(N)$ since it requires\n",
    "        $N$ multiplications and additions\n",
    "    -   $A x$ for $A\\in\\mathbb{R}^{N\\times N},x\\in\\mathbb{R}^N$ is\n",
    "        $O(N^2)$ since it requires $N$ dot products, each $O(N)$\n",
    "\n",
    "## Computational Complexity\n",
    "\n",
    "Ask yourself whether the following is a **computationally expensive**\n",
    "operation as the matrix **size increases**\n",
    "\n",
    "-   Multiplying two matrices?\n",
    "    -   *Answer*: It depends. Multiplying two diagonal matrices is\n",
    "        trivial.\n",
    "-   Solving a linear system of equations?\n",
    "    -   *Answer*: It depends. If the matrix is the identity, the\n",
    "        solution is the vector itself.\n",
    "-   Finding the eigenvalues of a matrix?\n",
    "    -   *Answer*: It depends. The eigenvalues of a triangular matrix are\n",
    "        the diagonal elements.\n",
    "\n",
    "## Numerical Precision\n",
    "\n",
    "**Machine Epsilon**\n",
    "\n",
    "For a given datatype, $\\epsilon$ is defined as\n",
    "$\\epsilon = \\min_{\\delta > 0} \\left\\{ \\delta : 1 + \\delta > 1 \\right\\}$\n",
    "\n",
    "-   Computers have finite precision. 64-bit typical, but 32-bit on GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "machine epsilon for float64 = 2.220446049250313e-16\n",
      "1 + eps/2 == 1? True\n",
      "machine epsilon for float32 = 1.1920928955078125e-07"
     ]
    }
   ],
   "source": [
    "print(f\"machine epsilon for float64 = {jnp.finfo(jnp.float64).eps}\")\n",
    "print(f\"1 + eps/2 == 1? {1.0 + 1.1e-16 == 1.0}\")\n",
    "print(f\"machine epsilon for float32 = {jnp.finfo(jnp.float32).eps}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix Structure\n",
    "\n",
    "## Matrix Structure\n",
    "\n",
    "-   A key principle is to ensure you don’t lose “structure”\n",
    "    -   e.g. if sparse, operations should keep it sparse if possible\n",
    "    -   If triangular, then use appropriate algorithms instead of\n",
    "        converting back to a dense matrix\n",
    "-   Key structure is:\n",
    "    -   Symmetry, diagonal, tridiagonal, banded, sparse,\n",
    "        positive-definite\n",
    "-   The worse operations for losing structure are matrix multiplication\n",
    "    and inversion\n",
    "\n",
    "## Example Losing Sparsity\n",
    "\n",
    "-   Here the density increases substantially\n",
    "-   We use NumPy for sparse matrix creation (JAX sparse support is\n",
    "    experimental)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Non-zeros in A: 45\n",
      "Non-zeros in inv(A): 100"
     ]
    }
   ],
   "source": [
    "# Create sparse random matrix using scipy\n",
    "np.random.seed(42)\n",
    "A_sp = sp.random(10, 10, density=0.45, format='csr')\n",
    "print(f\"Non-zeros in A: {A_sp.nnz}\")\n",
    "\n",
    "# Invert (must convert to dense)\n",
    "A_dense = A_sp.toarray()\n",
    "invA_dense = jnp.linalg.inv(A_dense)\n",
    "# Count non-zeros (threshold for numerical zeros)\n",
    "invA_nnz = jnp.sum(jnp.abs(invA_dense) > 1e-10)\n",
    "print(f\"Non-zeros in inv(A): {invA_nnz}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Losing Tridiagonal Structure\n",
    "\n",
    "-   An even more extreme example. Tridiagonal has roughly $3N$ nonzeros.\n",
    "    Inverses are dense $N^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Inverse of tridiagonal (all elements non-zero):\n",
      "[[ 1.2909946e+00 -3.2795697e-01  4.1666660e-02 -5.3763431e-03\n",
      "   6.7204301e-04]\n",
      " [-1.6397849e-01  1.3118279e+00 -1.6666664e-01  2.1505373e-02\n",
      "  -2.6881720e-03]\n",
      " [ 2.0833330e-02 -1.6666664e-01  1.2916665e+00 -1.6666664e-01\n",
      "   2.0833334e-02]\n",
      " [-2.6881718e-03  2.1505374e-02 -1.6666666e-01  1.3118279e+00\n",
      "  -1.6397850e-01]\n",
      " [ 6.7204307e-04 -5.3763445e-03  4.1666668e-02 -3.2795700e-01\n",
      "   1.2909946e+00]]"
     ]
    }
   ],
   "source": [
    "N = 5\n",
    "# Create tridiagonal matrix\n",
    "lower = jnp.concatenate([jnp.full(N-2, 0.1), jnp.array([0.2])])\n",
    "diag = jnp.full(N, 0.8)\n",
    "upper = jnp.concatenate([jnp.array([0.2]), jnp.full(N-2, 0.1)])\n",
    "\n",
    "# Build full matrix for inversion\n",
    "A_tri = jnp.diag(diag) + jnp.diag(lower, -1) + jnp.diag(upper, 1)\n",
    "print(\"Inverse of tridiagonal (all elements non-zero):\")\n",
    "print(jnp.linalg.inv(A_tri))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forming the Covariance and/or Gram Matrix\n",
    "\n",
    "-   Another common example is $A^T A$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Sparsity of A: 31.50%\n",
      "Sparsity of A'A: 85.94%"
     ]
    }
   ],
   "source": [
    "A_sp = sp.random(20, 21, density=0.3, format='csr')\n",
    "print(f\"Sparsity of A: {A_sp.nnz / (20*20):.2%}\")\n",
    "ATA = A_sp.T @ A_sp\n",
    "print(f\"Sparsity of A'A: {ATA.nnz / (21*21):.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specialized Algorithms\n",
    "\n",
    "-   Besides sparsity/storage, the real loss is you miss out on\n",
    "    algorithms\n",
    "-   We’ll compare dense vs. sparse vs. tridiagonal solvers\n",
    "\n",
    "## Compare Dense vs. Sparse vs. Tridiagonal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Tridiagonal (Lineax): 1958.95 ms\n",
      "Dense (JAX): 190.73 ms\n",
      "Speedup: 0.1x"
     ]
    }
   ],
   "source": [
    "N = 1000\n",
    "key, subkey = jax.random.split(key)\n",
    "b = jax.random.uniform(subkey, (N,))\n",
    "\n",
    "# Create tridiagonal matrix\n",
    "lower_diag = jnp.concatenate([jnp.full(N-2, 0.1), jnp.array([0.2])])\n",
    "main_diag = jnp.full(N, 0.8)\n",
    "upper_diag = jnp.concatenate([jnp.array([0.2]), jnp.full(N-2, 0.1)])\n",
    "\n",
    "# Lineax tridiagonal operator (uses parallel scan, O(N))\n",
    "A_tri_op = lx.TridiagonalLinearOperator(main_diag, lower_diag, upper_diag)\n",
    "start = time.perf_counter()\n",
    "x_tri = lx.linear_solve(A_tri_op, b).value\n",
    "x_tri.block_until_ready()\n",
    "tri_time = time.perf_counter() - start\n",
    "\n",
    "# Dense solve (O(N^3))\n",
    "A_dense = jnp.diag(main_diag) + jnp.diag(lower_diag, -1) + jnp.diag(upper_diag, 1)\n",
    "start = time.perf_counter()\n",
    "x_dense = jnp.linalg.solve(A_dense, b)\n",
    "x_dense.block_until_ready()\n",
    "dense_time = time.perf_counter() - start\n",
    "\n",
    "print(f\"Tridiagonal (Lineax): {tri_time*1000:.2f} ms\")\n",
    "print(f\"Dense (JAX): {dense_time*1000:.2f} ms\")\n",
    "print(f\"Speedup: {dense_time/tri_time:.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key insight**: Lineax uses a direct parallel scan solver (O(N)), much\n",
    "faster than dense solve (O(N^3))\n",
    "\n",
    "## Triangular Matrices and Back/Forward Substitution\n",
    "\n",
    "-   A key example of a better algorithm is for triangular matrices\n",
    "-   Upper or lower triangular matrices can be solved in $O(N^2)$ instead\n",
    "    of $O(N^3)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Solution: [0.         0.         0.33333334]"
     ]
    }
   ],
   "source": [
    "b_small = jnp.array([1.0, 2.0, 3.0])\n",
    "U = jnp.array([[1.0, 2.0, 3.0],\n",
    "               [0.0, 5.0, 6.0],\n",
    "               [0.0, 0.0, 9.0]])\n",
    "x = jla.solve_triangular(U, b_small, lower=False)\n",
    "print(f\"Solution: {x}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backwards Substitution Example\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "U x &= b\\\\\n",
    "U &\\equiv \\begin{bmatrix}\n",
    "3 & 1 \\\\\n",
    "0 & 2 \\\\\n",
    "\\end{bmatrix}, \\quad b = \\begin{bmatrix}\n",
    "7 \\\\\n",
    "2 \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Solving bottom row for $x_2$\n",
    "\n",
    "$$\n",
    "2 x_2 = 2,\\quad x_2 = 1\n",
    "$$\n",
    "\n",
    "Move up a row, solving for $x_1$, substituting for $x_2$\n",
    "\n",
    "$$\n",
    "3 x_1 + 1 x_2 = 7,\\quad 3 x_1 + 1 \\times 1 =  7,\\quad x_1 = 2\n",
    "$$\n",
    "\n",
    "Generalizes to many rows. For $L$ it is “forward substitution”\n",
    "\n",
    "# Factorizations\n",
    "\n",
    "## Factorizing matrices\n",
    "\n",
    "-   Just like you can factor $6 = 2 \\cdot 3$, you can factor matrices\n",
    "-   Unlike integers, you have more choice over the properties of the\n",
    "    factors\n",
    "-   Many operations (e.g., solving systems of equations, finding\n",
    "    eigenvalues, inverting, finding determinants) have a factorization\n",
    "    done internally\n",
    "    -   Instead you can often just find the factorization and reuse it\n",
    "-   Key factorizations: LU, QR, Cholesky, SVD, Schur, Eigenvalue\n",
    "\n",
    "## LU(P) Decompositions\n",
    "\n",
    "-   We can “factor” any square $A$ into $P A = L U$ for triangular $L$\n",
    "    and $U$. P is for partial-pivoting\n",
    "-   If invertible, then a $A = L U$ exists, but may not be numerically\n",
    "    stable without pivoting\n",
    "-   Returns explicit matrices P, L, U (not a factorization object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "P @ A ≈ L @ U? True"
     ]
    }
   ],
   "source": [
    "N_lu = 4\n",
    "key, subkey = jax.random.split(key)\n",
    "A = jax.random.uniform(subkey, (N_lu, N_lu))\n",
    "key, subkey = jax.random.split(key)\n",
    "b_lu = jax.random.uniform(subkey, (N_lu,))\n",
    "\n",
    "# LU factorization returns explicit matrices\n",
    "P, L, U = jla.lu(A)\n",
    "print(f\"P @ A ≈ L @ U? {jnp.allclose(P @ A, L @ U)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using LU Factorization\n",
    "\n",
    "-   To solve, use triangular solves manually or use `jnp.linalg.solve`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "LU solution: [-2.7407506   0.50105435  2.7111826   1.1074696 ]\n",
      "Direct solution: [-2.7407506   0.50105435  2.7111826   1.1074696 ]\n",
      "Solutions match? True"
     ]
    }
   ],
   "source": [
    "# Manual solve using LU: solve L(Ux) = Pb\n",
    "y = jla.solve_triangular(L, P @ b_lu, lower=True)\n",
    "x_lu = jla.solve_triangular(U, y, lower=False)\n",
    "\n",
    "# Direct solve for comparison\n",
    "x_direct = jnp.linalg.solve(A, b_lu)\n",
    "\n",
    "print(f\"LU solution: {x_lu}\")\n",
    "print(f\"Direct solution: {x_direct}\")\n",
    "print(f\"Solutions match? {jnp.allclose(x_lu, x_direct)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LU Decompositions and Systems of Equations\n",
    "\n",
    "-   Pivoting is typically implied when talking about “LU”\n",
    "-   Used in the default solve algorithm (without more structure)\n",
    "-   Solving systems of equations with triangular matrices: for\n",
    "    $A x = L U x = b$\n",
    "    1.  Define $y = U x$\n",
    "    2.  Solve $L y = P b$ for $y$ and $U x = y$ for $x$\n",
    "-   Since both are triangular, process is $O(N^2)$ (but LU itself\n",
    "    $O(N^3)$)\n",
    "-   Could be used to find `inv`\n",
    "    -   $A = L U$ then $A A^{-1} = I = L U A^{-1} = I$\n",
    "    -   Solve for $Y$ in $L Y = P$, then solve $U A^{-1} = Y$\n",
    "-   Tight connection to textbook Gaussian elimination (including\n",
    "    pivoting)\n",
    "\n",
    "## Cholesky\n",
    "\n",
    "-   LU is for general invertible matrices, but it doesn’t use\n",
    "    positive-definiteness or symmetry\n",
    "-   The Cholesky is the right factorization for positive-definite\n",
    "    matrices\n",
    "-   $A = L L^T$ for lower triangular $L$ (or $A = U^T U$ for upper\n",
    "    triangular)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "A is symmetric? True"
     ]
    }
   ],
   "source": [
    "N_chol = 500\n",
    "key, subkey = jax.random.split(key)\n",
    "B = jax.random.uniform(subkey, (N_chol, N_chol))\n",
    "A_pd = B.T @ B  # Easy way to generate positive definite matrix\n",
    "print(f\"A is symmetric? {jnp.allclose(A_pd, A_pd.T)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Cholesky"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Direct solve: 176.92 ms\n",
      "Cholesky solve: 2.29 ms\n",
      "Speedup: 77.4x"
     ]
    }
   ],
   "source": [
    "key, subkey = jax.random.split(key)\n",
    "b_chol = jax.random.uniform(subkey, (N_chol,))\n",
    "\n",
    "# Cholesky factorization\n",
    "L_chol = jla.cholesky(A_pd, lower=True)\n",
    "\n",
    "# Solve using Cholesky\n",
    "y_chol = jla.solve_triangular(L_chol, b_chol, lower=True)\n",
    "x_chol = jla.solve_triangular(L_chol.T, y_chol, lower=False)\n",
    "\n",
    "# Direct solve (doesn't know it's positive definite)\n",
    "start = time.perf_counter()\n",
    "x_direct = jnp.linalg.solve(A_pd, b_chol)\n",
    "x_direct.block_until_ready()\n",
    "direct_time = time.perf_counter() - start\n",
    "\n",
    "# Cholesky solve\n",
    "start = time.perf_counter()\n",
    "L_chol = jla.cholesky(A_pd, lower=True)\n",
    "y_chol = jla.solve_triangular(L_chol, b_chol, lower=True)\n",
    "x_chol = jla.solve_triangular(L_chol.T, y_chol, lower=False)\n",
    "x_chol.block_until_ready()\n",
    "chol_time = time.perf_counter() - start\n",
    "\n",
    "print(f\"Direct solve: {direct_time*1000:.2f} ms\")\n",
    "print(f\"Cholesky solve: {chol_time*1000:.2f} ms\")\n",
    "print(f\"Speedup: {direct_time/chol_time:.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eigen Decomposition\n",
    "\n",
    "-   For square, symmetric, non-singular matrix $A$ factor into\n",
    "\n",
    "$$\n",
    "A = Q \\Lambda Q^{-1}\n",
    "$$\n",
    "\n",
    "-   $Q$ is a matrix of eigenvectors, $\\Lambda$ is a diagonal matrix of\n",
    "    paired eigenvalues\n",
    "-   For symmetric matrices, the eigenvectors are orthogonal and\n",
    "    $Q^{-1} Q = Q^T Q = I$ which form an orthonormal basis\n",
    "-   Orthogonal matrices can be thought of as rotations without\n",
    "    stretching\n",
    "-   More general matrices all have a Singular Value Decomposition (SVD)\n",
    "-   With symmetric $A$, an interpretation of $A x$ is that we can first\n",
    "    rotate $x$ into the $Q$ basis, then stretch by $\\Lambda$, then\n",
    "    rotate back\n",
    "\n",
    "## Calculating the Eigen Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "||Q Λ Q^-1 - A||: 7.66e-07\n",
      "||Q Λ Q^T - A||: 6.90e-07"
     ]
    }
   ],
   "source": [
    "key, subkey = jax.random.split(key)\n",
    "A_sym = jax.random.uniform(subkey, (5, 5))\n",
    "A_sym = (A_sym + A_sym.T) / 2  # Make symmetric\n",
    "\n",
    "# Eigenvalue decomposition\n",
    "eigenvalues, Q = jnp.linalg.eigh(A_sym)  # eigh for symmetric/Hermitian\n",
    "Lambda = jnp.diag(eigenvalues)\n",
    "\n",
    "print(f\"||Q Λ Q^-1 - A||: {jnp.linalg.norm(Q @ Lambda @ jnp.linalg.inv(Q) - A_sym):.2e}\")\n",
    "print(f\"||Q Λ Q^T - A||: {jnp.linalg.norm(Q @ Lambda @ Q.T - A_sym):.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eigendecompositions and Matrix Powers\n",
    "\n",
    "-   Can be used to find $A^t$ for large $t$ (e.g. for Markov chains)\n",
    "    -   $P^t$, i.e. $P \\cdot P \\cdot \\ldots \\cdot P$ for $t$ times\n",
    "    -   $P = Q \\Lambda Q^{-1}$ then $P^t = Q \\Lambda^t Q^{-1}$ where\n",
    "        $\\Lambda^t$ is just the pointwise power\n",
    "-   Related can find matrix exponential $e^A$ for square matrices\n",
    "    -   $e^A = Q e^\\Lambda Q^{-1}$ where $e^\\Lambda$ is just the\n",
    "        pointwise exponential\n",
    "    -   Useful for solving differential equations, e.g. $y' = A y$ for\n",
    "        $y(0) = y_0$ is $y(t) = e^{A t} y_0$\n",
    "\n",
    "## More on Factorizations\n",
    "\n",
    "-   Plenty more used in different circumstances. Start by looking at\n",
    "    structure\n",
    "-   Usually have some connection to textbook algorithms, for example LU\n",
    "    is Gaussian elimination with pivoting and QR is Gram-Schmidt Process\n",
    "-   Just as shortcuts can be done with sparse matrices in textbook\n",
    "    examples, direct sparse methods can be faster given enough sparsity\n",
    "    -   But don’t assume sparsity will be faster. Often slower unless\n",
    "        matrices are big and especially sparse\n",
    "    -   Dense algorithms on GPUs can be very fast because of parallelism\n",
    "-   Keep in mind that barring numerical roundoff issues, these are\n",
    "    “exact” methods. They don’t become more accurate with more\n",
    "    iterations\n",
    "\n",
    "## Sparse Direct Solvers: The SciPy Fallback\n",
    "\n",
    "-   **JAX limitation**: No native direct sparse solver (like\n",
    "    UMFPACK/SuperLU)\n",
    "-   For sparse systems, we fall back to SciPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Solved sparse system of size 1000x1000 with 10987 non-zeros\n",
      "Residual: 1.71e-14"
     ]
    }
   ],
   "source": [
    "# Create sparse system\n",
    "N_sparse = 1000\n",
    "A_sparse = sp.random(N_sparse, N_sparse, density=0.01, format='csr')\n",
    "A_sparse = A_sparse + sp.eye(N_sparse) * 10  # Make diagonally dominant\n",
    "b_sparse = np.random.rand(N_sparse)\n",
    "\n",
    "# Solve using SciPy's sparse solver (uses UMFPACK/SuperLU)\n",
    "x_sparse = spla.spsolve(A_sparse, b_sparse)\n",
    "print(f\"Solved sparse system of size {N_sparse}x{N_sparse} with {A_sparse.nnz} non-zeros\")\n",
    "print(f\"Residual: {np.linalg.norm(A_sparse @ x_sparse - b_sparse):.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: For production sparse linear solves, use SciPy or interface\n",
    "with PETSc, not JAX\n",
    "\n",
    "## Large Scale Systems of Equations\n",
    "\n",
    "-   Packages that solve BIG problems with “direct methods” include\n",
    "    [MUMPS](https://mumps-solver.org/index.php),\n",
    "    [Pardiso](https://www.intel.com/content/www/us/en/docs/onemkl/developer-reference-c/2023-0/onemkl-pardiso-parallel-direct-sparse-solver-iface.html),\n",
    "    [UMFPACK](https://en.wikipedia.org/wiki/UMFPACK), and many others\n",
    "-   Sparse solvers are bread-and-butter scientific computing, so they\n",
    "    can crush huge problems, parallelize on a cluster, etc.\n",
    "-   But for smaller problems they may not be ideal. Profile and test,\n",
    "    and only if you need it.\n",
    "-   On Python: scipy has many built in (UMFPACK, SuperLU, etc.) and many\n",
    "    wrappers exist. Same with Matlab\n",
    "\n",
    "## Preview of Conditioning\n",
    "\n",
    "-   It will turn out that for iterative methods, a different style of\n",
    "    algorithm, it is often necessary to multiply by a matrix to\n",
    "    transform the problem\n",
    "-   The ideal transform would be the matrix’s inverse, which requires a\n",
    "    full factorization\n",
    "-   But instead, you can do only part of the way towards the\n",
    "    factorization. e.g., part of the way on gaussian elimination\n",
    "-   Called “Incomplete Cholesky”, “Incomplete LU”, etc.\n",
    "\n",
    "# Continuous Time Markov Chains\n",
    "\n",
    "## Markov Chains Transitions in Continuous Time\n",
    "\n",
    "-   For a discrete number of states, we cannot have instantaneous\n",
    "    transitions between states or it ceases to be measurable\n",
    "-   Instead: intensity of switching from state $i$ to $j$ as a $q_{ij}$\n",
    "    where\n",
    "\n",
    "$$\n",
    "\\mathbb P \\{ X(t + \\Delta) = j  \\,|\\, X(t) \\} = \\begin{cases} q_{ij} \\Delta + o(\\Delta) & i \\neq j\\\\\n",
    "                                                              1 + q_{ii} \\Delta + o(\\Delta) & i = j \\end{cases}\n",
    "$$\n",
    "\n",
    "-   With $o(\\Delta)$ is [little-o\n",
    "    notation](https://en.wikipedia.org/wiki/Big_O_notation#Little-o_notation).\n",
    "    That is, $\\lim_{\\Delta\\to 0} o(\\Delta)/\\Delta = 0$.\n",
    "\n",
    "## Intensity Matrix\n",
    "\n",
    "-   $Q_{ij} = q_{ij}$ for $i \\neq j$ and\n",
    "    $Q_{ii} = -\\sum_{j \\neq i} q_{ij}$\n",
    "-   Rows sum to 0\n",
    "-   For example, consider a counting process\n",
    "\n",
    "$$\n",
    "Q = \\begin{bmatrix} -0.1 & 0.1  & 0 & 0 & 0 & 0\\\\\n",
    "                    0.1  &-0.2  & 0.1 &  0 & 0 & 0\\\\\n",
    "                    0 & 0.1 & -0.2 & 0.1 & 0 & 0\\\\\n",
    "                    0 & 0 & 0.1 & -0.2 & 0.1 & 0\\\\\n",
    "                    0 & 0 & 0 & 0.1 & -0.2 & 0.1\\\\\n",
    "                    0 & 0 & 0 & 0 & 0.1 & -0.1\\\\\n",
    "    \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "## Probability Dynamics\n",
    "\n",
    "-   The $Q$ is the [infinitesimal\n",
    "    generator](https://en.wikipedia.org/wiki/Infinitesimal_generator_(stochastic_processes))\n",
    "    of the stochastic process.\n",
    "-   Let $\\pi(t) \\in \\mathbb{R}^N$ with\n",
    "    $\\pi_i(t) \\equiv \\mathbb{P}[X_t = i\\,|\\,X_0]$\n",
    "-   Then the probability distribution evolution (Fokker-Planck or KFE),\n",
    "    is\n",
    "\n",
    "$$\n",
    "\\frac{d}{dt} \\pi(t) = \\pi(t) Q,\\quad \\text{ given }\\pi(0)\n",
    "$$\n",
    "\n",
    "-   Or, often written as $\\frac{d}{dt}  \\pi(t) = Q^{\\top} \\cdot \\pi(t)$,\n",
    "    i.e. in terms of the “adjoint” of the linear operator $Q$\n",
    "-   A steady state is then a solution to $Q^{\\top} \\cdot \\bar{\\pi} = 0$\n",
    "    -   i.e., the $\\bar{\\pi}$ left-eigenvector associated with\n",
    "        eigenvalue 0 (i.e. $\\bar{\\pi} Q = 0\\times \\bar{\\pi}$)\n",
    "\n",
    "## Setting up a Counting Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Q matrix:\n",
      "[[-0.1  0.1  0.   0.   0.   0. ]\n",
      " [ 0.1 -0.2  0.1  0.   0.   0. ]\n",
      " [ 0.   0.1 -0.2  0.1  0.   0. ]\n",
      " [ 0.   0.   0.1 -0.2  0.1  0. ]\n",
      " [ 0.   0.   0.   0.1 -0.2  0.1]\n",
      " [ 0.   0.   0.   0.   0.1 -0.1]]"
     ]
    }
   ],
   "source": [
    "alpha = 0.1\n",
    "N_ctmc = 6\n",
    "\n",
    "# Create tridiagonal Q matrix\n",
    "lower_ctmc = jnp.full(N_ctmc-1, alpha)\n",
    "main_ctmc = jnp.concatenate([jnp.array([-alpha]),\n",
    "                             jnp.full(N_ctmc-2, -2*alpha),\n",
    "                             jnp.array([-alpha])])\n",
    "upper_ctmc = jnp.full(N_ctmc-1, alpha)\n",
    "\n",
    "# Build dense matrix for display\n",
    "Q = jnp.diag(main_ctmc) + jnp.diag(lower_ctmc, -1) + jnp.diag(upper_ctmc, 1)\n",
    "print(\"Q matrix:\")\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the Stationary Distribution\n",
    "\n",
    "-   There will always be at least one eigenvalue of 0, and the\n",
    "    corresponding eigenvector is the stationary distribution\n",
    "-   We use dense eigenvalue decomposition here (for iterative methods,\n",
    "    see next lecture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "output-location": "column"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Eigenvalues:\n",
      "[-3.7320518e-01 -3.0000022e-01 -2.0000000e-01 -9.9999972e-02\n",
      " -1.0465228e-08 -2.6794920e-02]\n",
      "\n",
      "Stationary distribution:\n",
      "[0.16666669 0.16666669 0.16666669 0.16666666 0.16666664 0.16666664]"
     ]
    }
   ],
   "source": [
    "# Eigenvalue decomposition of Q^T\n",
    "eigenvalues, eigenvectors = jnp.linalg.eig(Q.T)\n",
    "\n",
    "# Find eigenvector corresponding to eigenvalue ≈ 0\n",
    "idx = jnp.argmin(jnp.abs(eigenvalues))\n",
    "pi_stationary = eigenvectors[:, idx].real\n",
    "pi_stationary = pi_stationary / jnp.sum(pi_stationary)\n",
    "\n",
    "print(f\"Eigenvalues:\\n{eigenvalues.real}\")\n",
    "print(f\"\\nStationary distribution:\")\n",
    "print(pi_stationary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Generator in a Bellman Equation\n",
    "\n",
    "-   Let $r \\in \\mathbb{R}^N$ be a vector of payoffs in each state, and\n",
    "    $\\rho > 0$ a discount rate\n",
    "-   Then we can use the $Q$ generator as a simple Bellman Equation\n",
    "    (using the Kolmogorov Backwards Equation) to find the value $v$ in\n",
    "    each state\n",
    "\n",
    "$$\n",
    "\\rho v = r + Q v\n",
    "$$\n",
    "\n",
    "-   Rearranging, $(\\rho I - Q) v = r$\n",
    "\n",
    "## Implementing the Bellman Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "output-location": "column"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Value function:\n",
      "[ 38.153847  57.230774  84.92308  115.076935 142.76924  161.84616 ]"
     ]
    }
   ],
   "source": [
    "rho = 0.05\n",
    "r = jnp.linspace(0.0, 10.0, N_ctmc)\n",
    "\n",
    "# Solve (rho * I - Q) v = r\n",
    "A_bellman = rho * jnp.eye(N_ctmc) - Q\n",
    "v = jnp.linalg.solve(A_bellman, r)\n",
    "\n",
    "print(f\"Value function:\")\n",
    "print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Teaser**: Can we use iterative methods to avoid forming the full\n",
    "matrix? See next lecture!\n",
    "\n",
    "## References"
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "path": "/home/runner/work/grad_econ_ML/grad_econ_ML/.venv/share/jupyter/kernels/python3"
  },
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": "3"
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 }
}
