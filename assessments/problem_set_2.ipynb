{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ECON622: Problem Set 2\n",
    "\n",
    "Jesse Perla, UBC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Student Name/Number: Wan-Chiao Chang / 17065012"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages\n",
    "\n",
    "Add whatever packages you wish here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import pandas as pd\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import grad, hessian\n",
    "from jax import random\n",
    "import optax\n",
    "import optimistix as otx\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "\n",
    "The trace of the Hessian matrix is useful in a variety of applications\n",
    "in statistics, econometrics, and stochastic processes. It can also be\n",
    "used to regularize a loss function.\n",
    "\n",
    "For a function $f:\\mathbb{R}^N\\to\\mathbb{R}$, denote the Hessian as\n",
    "$\\nabla^2 f(x) \\in \\mathbb{R}^{N\\times N}$.\n",
    "\n",
    "It can be shown that for some mean zero, unit variance random vectors\n",
    "$v\\in\\mathbb{R}^N$ with $\\mathbb{E}(v) = 0$ and\n",
    "$\\mathbb{E}(v v^{\\top}) = I$ the trace of the Hessian fulfills\n",
    "\n",
    "$$\n",
    "\\mathrm{Tr}(\\nabla^2 f(x)) = \\mathbb{E}\\left[v^{\\top} \\nabla^2 f(x)\\, v\\right]\n",
    "$$\n",
    "\n",
    "Which leads to a random algorithm by sampling $M$ vectors\n",
    "$v_1,\\ldots,v_M$ and using the Monte Carlo approximation of the\n",
    "expectation, called the [Hutchinson Trace\n",
    "Estimator](https://www.tandfonline.com/doi/abs/10.1080/03610918908812806)\n",
    "\n",
    "$$\n",
    "\\mathrm{Tr}(\\nabla^2 f(x)) \\approx \\frac{1}{M} \\sum_{m=1}^M v_m^{\\top} \\nabla^2 f(x)\\, v_m\n",
    "$$\n",
    "\n",
    "### Question 1.1\n",
    "\n",
    "Now, let’s take the function $f(x) = \\frac{1}{2}x^{\\top} P x$, which is\n",
    "a quadratic form and where we know that $\\nabla^2 f(x) = P$.\n",
    "\n",
    "The following code finds the trace of the Hessian, which is equivalently\n",
    "just the sum of the diagonal of $P$ in this simple function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10240.816\n",
      "10240.816\n"
     ]
    }
   ],
   "source": [
    "key = jax.random.PRNGKey(0)\n",
    "\n",
    "N = 100  # Dimension of the matrix\n",
    "A = jax.random.normal(key, (N, N))\n",
    "# Create a positive-definite matrix P by forming A^T * A\n",
    "P = jnp.dot(A.T, A)\n",
    "def f(x):\n",
    "    return 0.5 * jnp.dot(x.T, jnp.dot(P, x))\n",
    "x = jax.random.normal(key, (N,))\n",
    "print(jnp.trace(jax.hessian(f)(x)))\n",
    "print(jnp.diag(P).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, instead of calculating the whole Hessian, use a [Hessian-vector\n",
    "product in\n",
    "JAX](https://jax.readthedocs.io/en/latest/notebooks/autodiff_cookbook.html#hessian-vector-products-using-both-forward-and-reverse-mode)\n",
    "and the approximation above with $M$ draws of random vectors to\n",
    "calculate an approximation of the trace of the Hessian. Increase the\n",
    "numbers of $M$ to see what the variance of the estimator is, comparing\n",
    "to the above closed-form solution for this quadratic.\n",
    "\n",
    "Hint: you will want to do Forward-over-Reverse mode differentiation for\n",
    "this (i.e. the `vjp` gives a pullback function for first derivative,\n",
    "then differentiate that new function. Given that it would then be\n",
    "$\\mathbb{R}^N \\to \\mathbb{R}^N$, it makes sense to use forward mode with\n",
    "a `jvp`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jax.grad (Reverse) computes the gradient\n",
    "# jax.jvp (Forward) differentiates the gradient in direction v\n",
    "def hessian_vector_product(f, x, v):\n",
    "    # jvp returns (primals, tangents). We only need the tangent (the derivative part).\n",
    "    primals, tangent = jax.jvp(jax.grad(f), (x,), (v,))\n",
    "    return tangent\n",
    "\n",
    "# Hutchinson Trace Estimator Function\n",
    "def estimate_trace(f, x, M, key):\n",
    "    estimates = []\n",
    "    \n",
    "    # Generate M random vectors (standard normal distribution)\n",
    "    vs = jax.random.normal(key, (M, N))\n",
    "    \n",
    "    for i in range(M):\n",
    "        v = vs[i]\n",
    "        # Compute Hv (Hessian-vector product)\n",
    "        Hv = hessian_vector_product(f, x, v)\n",
    "        # Compute v^T * Hv\n",
    "        val = jnp.dot(v.T, Hv)\n",
    "        estimates.append(val)\n",
    "    \n",
    "    return jnp.mean(jnp.array(estimates))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "537e67b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating Trace using Hutchinson's Method:\n",
      "M = 10    | Est: 10379.485 | Error: 1.35%\n",
      "M = 100   | Est: 10214.259 | Error: 0.26%\n",
      "M = 500   | Est: 10146.236 | Error: 0.92%\n",
      "M = 1000  | Est: 10223.255 | Error: 0.17%\n",
      "M = 5000  | Est: 10229.800 | Error: 0.11%\n"
     ]
    }
   ],
   "source": [
    "# We try increasing values of M to see the variance decrease\n",
    "true_trace = jnp.diag(P).sum()\n",
    "M_values = [10, 100, 500, 1000, 5000]\n",
    "\n",
    "print(\"Estimating Trace using Hutchinson's Method:\")\n",
    "for M in M_values:\n",
    "    # Use a new key for the sampling step\n",
    "    key, subkey = jax.random.split(key)\n",
    "    \n",
    "    approx_trace = estimate_trace(f, x, M, subkey)\n",
    "    error = abs(approx_trace - true_trace)\n",
    "    percent_error = (error / true_trace) * 100\n",
    "    \n",
    "    print(f\"M = {M:<5} | Est: {approx_trace:.3f} | Error: {percent_error:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.2 (Bonus)\n",
    "\n",
    "If you wish, you can play around with radically increase the size of the\n",
    "`N` and change the function itself. One suggestion is to move towards a\n",
    "sparse or even matrix-free $f(x)$ calculation so that the $P$ doesn’t\n",
    "itself need to materialize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension N: 100000\n",
      "True Trace (Analytical): 5000050000\n",
      "----------------------------------------\n",
      "Running Hutchinson Estimator with M=1000 samples...\n",
      "Estimated Trace:       4999923200\n",
      "Error:                 0.0025%\n"
     ]
    }
   ],
   "source": [
    "# --- 1. SETUP: RADICAL SCALE ---\n",
    "# We use N = 100,000. \n",
    "N = 100_000 \n",
    "key = jax.random.PRNGKey(42)\n",
    "\n",
    "# --- 2. DEFINE MATRIX-FREE FUNCTION ---\n",
    "# Use weighted sum of square function\n",
    "# Effectively, our \"Matrix\" P is a diagonal matrix where P_ii = i + 1\n",
    "def f(x):\n",
    "    # Create the \"diagonal\" weights on the fly (1, 2, ..., N)\n",
    "    weights = jnp.arange(1, N + 1, dtype=jnp.float32)\n",
    "    return 0.5 * jnp.sum(weights * (x ** 2))\n",
    "\n",
    "# Analytical True Trace: Sum of 1 to N -> N*(N+1)/2\n",
    "true_trace = (N * (N + 1)) / 2\n",
    "print(f\"Dimension N: {N}\")\n",
    "print(f\"True Trace (Analytical): {true_trace:.0f}\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# --- 3. HUTCHINSON ESTIMATOR (Matrix-Free) ---\n",
    "\n",
    "# Using the efficient Forward-over-Reverse (HvP) from the previous step\n",
    "def hessian_vector_product(f, x, v):\n",
    "    # jax.grad computes gradient (Reverse mode)\n",
    "    # jax.jvp computes directional derivative (Forward mode)\n",
    "    # This computes H*v without building H\n",
    "    return jax.jvp(jax.grad(f), (x,), (v,))[1]\n",
    "\n",
    "def hutchinson_estimate(f, x, M, key):\n",
    "    # Generate M random vectors (Rademacher or Normal)\n",
    "    # Normal (mean=0, var=1) is standard for Hutchinson\n",
    "    vs = jax.random.normal(key, (M, N))\n",
    "    \n",
    "    estimates = []\n",
    "    for i in range(M):\n",
    "        v = vs[i]\n",
    "        Hv = hessian_vector_product(f, x, v)\n",
    "        # v^T * Hv\n",
    "        estimates.append(jnp.dot(v, Hv))\n",
    "        \n",
    "    return jnp.mean(jnp.array(estimates))\n",
    "\n",
    "# --- 4. RUN THE ESTIMATION ---\n",
    "# Arbitrary input x (value doesn't matter for quadratic Hessian)\n",
    "x_input = jnp.ones(N)\n",
    "\n",
    "# Let's try with 1000 samples\n",
    "M = 1000\n",
    "key, subkey = jax.random.split(key)\n",
    "\n",
    "print(f\"Running Hutchinson Estimator with M={M} samples...\")\n",
    "est_trace = hutchinson_estimate(f, x_input, M, subkey)\n",
    "\n",
    "error_perc = 100 * abs(est_trace - true_trace) / true_trace\n",
    "\n",
    "print(f\"Estimated Trace:       {est_trace:.0f}\")\n",
    "print(f\"Error:                 {error_perc:.4f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "\n",
    "This section gives some hints on how to setup a differentiable\n",
    "likelihood function with implicit functions\n",
    "\n",
    "### Question 2.1\n",
    "\n",
    "The following code uses scipy to find the equilibrium price and demand\n",
    "for some simple supply and demand functions with embedded parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Equilibrium Price: 17.65\n",
      "Equilibrium Quantity: 91.60\n"
     ]
    }
   ],
   "source": [
    "from scipy.optimize import root_scalar\n",
    "\n",
    "# Define the demand function with power c\n",
    "def demand(P, c_d):\n",
    "    return 100 - 2 * P**c_d\n",
    "\n",
    "# Define the supply function with power f\n",
    "def supply(P, c_s):\n",
    "    return 5 * 3**(c_s * P)\n",
    "\n",
    "# Define the function to find the root of, including c and f\n",
    "def equilibrium(P, c_d, c_s):\n",
    "    return demand(P, c_d) - supply(P, c_s)\n",
    "\n",
    "# Use root_scalar to find the equilibrium price\n",
    "def find_equilibrium(c_d, c_s):\n",
    "    result = root_scalar(equilibrium, args=(c_d, c_s), bracket=[0, 100], method='brentq')\n",
    "    return result.root, demand(result.root, c_d)\n",
    "\n",
    "# Example usage\n",
    "c_d = 0.5\n",
    "c_s = 0.15\n",
    "equilibrium_price, equilibrium_quantity = find_equilibrium(c_d, c_s)\n",
    "print(f\"Equilibrium Price: {equilibrium_price:.2f}\")\n",
    "print(f\"Equilibrium Quantity: {equilibrium_quantity:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, convert this to use JAX and\n",
    "[Optimistix](https://docs.kidger.site/optimistix/) for finding the root\n",
    "using `optimistix.root_find()`. Make sure you can jit the whole\n",
    "`find_equilibrium` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Equilibrium Price: 17.6464\n",
      "Equilibrium Quantity: 91.5985\n"
     ]
    }
   ],
   "source": [
    "def demand(P, c_d):\n",
    "    return 100 - 2 * jnp.power(P, c_d)\n",
    "\n",
    "def supply(P, c_s):\n",
    "    return 5 * jnp.power(3.0, c_s * P)\n",
    "\n",
    "def equilibrium_fn(P, args):\n",
    "    c_d, c_s = args\n",
    "    return demand(P, c_d) - supply(P, c_s)\n",
    "\n",
    "@jax.jit\n",
    "def find_equilibrium(c_d, c_s):\n",
    "    solver = otx.Bisection(rtol=1e-6, atol=1e-6)\n",
    "    \n",
    "    # Bisection in Optimistix requires the bracket in the options dict\n",
    "    # y0 is used as a dummy scalar to define the shape of the output (a scalar price)\n",
    "    options = dict(lower=0.0, upper=100.0)\n",
    "    \n",
    "    sol = otx.root_find(\n",
    "        fn=equilibrium_fn,\n",
    "        solver=solver,\n",
    "        y0=0.0,  # Tells Optimistix we are looking for a scalar root\n",
    "        args=(c_d, c_s),\n",
    "        options=options,\n",
    "        throw=False \n",
    "    )\n",
    "    \n",
    "    price = sol.value\n",
    "    quantity = demand(price, c_d)\n",
    "    \n",
    "    return price, quantity\n",
    "\n",
    "# --- Test Usage ---\n",
    "c_d_val = 0.5\n",
    "c_s_val = 0.15\n",
    "\n",
    "price, quantity = find_equilibrium(c_d_val, c_s_val)\n",
    "\n",
    "print(f\"Equilibrium Price: {price:.4f}\")\n",
    "print(f\"Equilibrium Quantity: {quantity:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.2\n",
    "\n",
    "Now, assume that you get a noisy signal on the price that fulfills that\n",
    "demand system.\n",
    "\n",
    "$$\n",
    "\\hat{p} \\sim \\mathcal{N}(p, \\sigma^2)\n",
    "$$\n",
    "\n",
    "In that case, the log likelihood for the Gaussian is\n",
    "\n",
    "$$\n",
    "\\log \\mathcal{L}(\\hat{p}\\,|\\,c_d, c_s, p) = -\\frac{1}{2} \\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2} (\\hat{p} - p)^2\n",
    "$$\n",
    "\n",
    "Or, if $p$ was implicitly defined by the equilibrium conditions as some\n",
    "$p(c_d, c_s)$ from above,\n",
    "\n",
    "$$\n",
    "\\log \\mathcal{L}(\\hat{p}\\,|\\,c_d, c_s) = -\\frac{1}{2} \\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2} (\\hat{p} - p(c_d, c_s))^2\n",
    "$$\n",
    "\n",
    "Then for some $\\sigma = 0.01$ we can calculate this log likelihood the\n",
    "above as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(2.2389274, dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def log_likelihood(p_hat, c_d, c_s, sigma):\n",
    "    p, x = find_equilibrium(c_d, c_s)\n",
    "    return -0.5 * np.log(2 * np.pi * sigma**2) - 0.5 * (p_hat - p)**2 / sigma**2\n",
    "\n",
    "c_d = 0.5\n",
    "c_s = 0.15\n",
    "sigma = 0.01\n",
    "p, x = find_equilibrium(c_d, c_s) # get the true value for simulation\n",
    "p_hat = p + np.random.normal(0, sigma) # simulate a noisy signal\n",
    "log_likelihood(p_hat, c_d, c_s, sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, take this code for the likelihood and convert it to JAX and jit.\n",
    "Use your function from Question 2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulated Observed Price (p_hat): 17.6626\n",
      "Log Likelihood: 2.3698\n"
     ]
    }
   ],
   "source": [
    "@jax.jit\n",
    "def log_likelihood(p_hat, c_d, c_s, sigma):\n",
    "    \"\"\"\n",
    "    Calculates the Gaussian log-likelihood for an observed price p_hat.\n",
    "    The true price p is implicitly defined by (c_d, c_s).\n",
    "    \"\"\"\n",
    "    # 1. Solve for the theoretical equilibrium price p\n",
    "    p, _ = find_equilibrium(c_d, c_s)\n",
    "    \n",
    "    # 2. Compute Gaussian log-likelihood components\n",
    "    # Formula: -0.5 * log(2πσ²) - 0.5 * (p_hat - p)² / σ²\n",
    "    term1 = -0.5 * jnp.log(2 * jnp.pi * jnp.power(sigma, 2))\n",
    "    term2 = -0.5 * jnp.power(p_hat - p, 2) / jnp.power(sigma, 2)\n",
    "    \n",
    "    return term1 + term2\n",
    "\n",
    "# --- Execution & Simulation ---\n",
    "# Parameters from the screenshot\n",
    "c_d_true = 0.5\n",
    "c_s_true = 0.15\n",
    "sigma_val = 0.01\n",
    "\n",
    "# Find true price for simulation\n",
    "p_true, _ = find_equilibrium(c_d_true, c_s_true)\n",
    "\n",
    "# Simulate a noisy signal using JAX's random keys\n",
    "key = jax.random.PRNGKey(0)\n",
    "p_hat = p_true + jax.random.normal(key) * sigma_val\n",
    "\n",
    "# Calculate Likelihood\n",
    "ll_value = log_likelihood(p_hat, c_d_true, c_s_true, sigma_val)\n",
    "\n",
    "print(f\"Simulated Observed Price (p_hat): {p_hat:.4f}\")\n",
    "print(f\"Log Likelihood: {ll_value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.3\n",
    "\n",
    "Use the function from the previous part and calculate the gradient with\n",
    "respect to `params` (i.e., `c_d` and `c_s`) using `grad` and JAX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "log_likelihood() missing 1 required positional argument: 'sigma'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m sigma_val = \u001b[32m0.01\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Calculate the gradient\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m gradients = \u001b[43mgrad_ll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams_init\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp_hat_observed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msigma_val\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mGradient with respect to c_d: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgradients[\u001b[32m0\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mGradient with respect to c_s: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgradients[\u001b[32m1\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "    \u001b[31m[... skipping hidden 9 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dropbox/Courses/2025-2026/econ_622_computational_economics/grad_econ_ML_notebooks/.venv/lib/python3.14/site-packages/jax/_src/api_util.py:303\u001b[39m, in \u001b[36m_argnums_partial\u001b[39m\u001b[34m(_fun, _dyn_argnums, _fixed_args, *dyn_args, **kwargs)\u001b[39m\n\u001b[32m    301\u001b[39m args = [\u001b[38;5;28mnext\u001b[39m(fixed_args_).val \u001b[38;5;28;01mif\u001b[39;00m x \u001b[38;5;129;01mis\u001b[39;00m sentinel \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m args]\n\u001b[32m    302\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(fixed_args_, sentinel) \u001b[38;5;129;01mis\u001b[39;00m sentinel\n\u001b[32m--> \u001b[39m\u001b[32m303\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_fun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: log_likelihood() missing 1 required positional argument: 'sigma'"
     ]
    }
   ],
   "source": [
    "# Define the gradient function with respect to the first argument (params)\n",
    "grad_ll = jax.grad(log_likelihood, argnums=0)\n",
    "\n",
    "# Setup test values\n",
    "params_init = (0.5, 0.15)  # (c_d, c_s)\n",
    "p_hat_observed = 18.51     # Simulated observation\n",
    "sigma_val = 0.01\n",
    "\n",
    "# Calculate the gradient\n",
    "gradients = grad_ll(p_hat_observed, params_init, sigma_val)\n",
    "\n",
    "print(f\"Gradient with respect to c_d: {gradients[0]:.6f}\")\n",
    "print(f\"Gradient with respect to c_s: {gradients[1]:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.4 (Bonus)\n",
    "\n",
    "You could try to run maximum likelihood estimation by using a\n",
    "gradient-based optimizer. You can use either\n",
    "[Optax](https://optax.readthedocs.io/) (standard for ML optimization) or\n",
    "[Optimistix](https://docs.kidger.site/optimistix/) with\n",
    "`optimistix.minimise()`.\n",
    "\n",
    "If you attempt this:\n",
    "\n",
    "-   Consider starting your optimization at the “pseudo-true” values with\n",
    "    the `c_s, c_d, sigma` you used to simulate the data and even start\n",
    "    with `p_hat = p`.\n",
    "-   You may find that it is a little too noisy with only the one\n",
    "    observation. If so, you could adapt your likelihood to take a vector\n",
    "    of $\\hat{p}$ instead. The likelihood of IID gaussians is a simple\n",
    "    variation on the above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Question 2.4 (Bonus): Maximum Likelihood Estimation with Gradient-Based Optimizer\n\n# --- 1. Define log-likelihood for multiple IID observations ---\n# For N IID Gaussian observations, the log-likelihood is:\n# LL = sum_i [ -0.5 * log(2πσ²) - 0.5 * (p_hat_i - p)² / σ² ]\n#    = -N/2 * log(2πσ²) - 1/(2σ²) * sum_i (p_hat_i - p)²\n\n@jax.jit\ndef neg_log_likelihood_multi(params, p_hat_vec, sigma):\n    \"\"\"\n    Negative log-likelihood for multiple IID observations.\n    params: (c_d, c_s) - parameters to estimate\n    p_hat_vec: array of observed prices\n    sigma: noise standard deviation (known)\n    \"\"\"\n    c_d, c_s = params\n    \n    # Solve for equilibrium price\n    p_eq, _ = find_equilibrium(c_d, c_s)\n    \n    # Number of observations\n    N = p_hat_vec.shape[0]\n    \n    # Log-likelihood for IID Gaussians\n    term1 = -0.5 * N * jnp.log(2 * jnp.pi * sigma**2)\n    term2 = -0.5 * jnp.sum((p_hat_vec - p_eq)**2) / sigma**2\n    \n    # Return NEGATIVE log-likelihood for minimization\n    return -(term1 + term2)\n\n\n# --- 2. Simulate data ---\n# True parameters\nc_d_true = 0.5\nc_s_true = 0.15\nsigma_true = 0.1  # Slightly larger sigma for more realistic noise\n\n# Get true equilibrium price\np_true, q_true = find_equilibrium(c_d_true, c_s_true)\nprint(f\"True parameters: c_d = {c_d_true}, c_s = {c_s_true}\")\nprint(f\"True equilibrium price: {p_true:.4f}\")\n\n# Generate multiple noisy observations\nkey = jax.random.PRNGKey(123)\nN_obs = 100  # Number of observations\np_hat_data = p_true + sigma_true * jax.random.normal(key, (N_obs,))\nprint(f\"Number of observations: {N_obs}\")\nprint(f\"Sample mean of observations: {jnp.mean(p_hat_data):.4f}\")\n\n\n# --- 3. MLE using Optax (gradient descent) ---\n# Initialize at values close to but not exactly at true values\nparams_init = jnp.array([0.6, 0.2])  # Starting guess: (c_d, c_s)\n\n# Setup optimizer (Adam with learning rate)\nlearning_rate = 0.001\noptimizer = optax.adam(learning_rate)\nopt_state = optimizer.init(params_init)\n\n# Gradient of negative log-likelihood w.r.t. params\ngrad_nll = jax.grad(neg_log_likelihood_multi, argnums=0)\n\n# Training loop\n@jax.jit\ndef update_step(params, opt_state, p_hat_vec, sigma):\n    grads = grad_nll(params, p_hat_vec, sigma)\n    updates, new_opt_state = optimizer.update(grads, opt_state, params)\n    new_params = optax.apply_updates(params, updates)\n    return new_params, new_opt_state, grads\n\n# Run optimization\nparams = params_init\nn_iterations = 2000\nprint_every = 500\n\nprint(\"\\n--- Starting Optax Optimization ---\")\nprint(f\"Initial params: c_d = {params[0]:.4f}, c_s = {params[1]:.4f}\")\nprint(f\"Initial NLL: {neg_log_likelihood_multi(params, p_hat_data, sigma_true):.4f}\")\n\nfor i in range(n_iterations):\n    params, opt_state, grads = update_step(params, opt_state, p_hat_data, sigma_true)\n    \n    if (i + 1) % print_every == 0:\n        nll = neg_log_likelihood_multi(params, p_hat_data, sigma_true)\n        print(f\"Iter {i+1:4d} | c_d: {params[0]:.4f}, c_s: {params[1]:.4f} | NLL: {nll:.4f} | grad: [{grads[0]:.4f}, {grads[1]:.4f}]\")\n\n# Final results\nprint(\"\\n--- Final Results ---\")\nprint(f\"Estimated c_d: {params[0]:.4f} (true: {c_d_true})\")\nprint(f\"Estimated c_s: {params[1]:.4f} (true: {c_s_true})\")\n\n# Check equilibrium price from estimated parameters\np_estimated, _ = find_equilibrium(params[0], params[1])\nprint(f\"Estimated equilibrium price: {p_estimated:.4f} (true: {p_true:.4f})\")\n\n# Compare to sample mean (which is MLE for Gaussian mean)\nprint(f\"Sample mean of observations: {jnp.mean(p_hat_data):.4f}\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "grad-econ-ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}