{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ECON622: Problem Set 2\n",
    "\n",
    "Jesse Perla, UBC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Student Name/Number: (doubleclick to edit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages\n",
    "\n",
    "Add whatever packages you wish here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import pandas as pd\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import grad, hessian\n",
    "from jax import random\n",
    "import optax\n",
    "import optimistix\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "\n",
    "The trace of the Hessian matrix is useful in a variety of applications\n",
    "in statistics, econometrics, and stochastic processes. It can also be\n",
    "used to regularize a loss function.\n",
    "\n",
    "For a function $f:\\mathbb{R}^N\\to\\mathbb{R}$, denote the Hessian as\n",
    "$\\nabla^2 f(x) \\in \\mathbb{R}^{N\\times N}$.\n",
    "\n",
    "It can be shown that for some mean zero, unit variance random vectors\n",
    "$v\\in\\mathbb{R}^N$ with $\\mathbb{E}(v) = 0$ and\n",
    "$\\mathbb{E}(v v^{\\top}) = I$ the trace of the Hessian fulfills\n",
    "\n",
    "$$\n",
    "\\mathrm{Tr}(\\nabla^2 f(x)) = \\mathbb{E}\\left[v^{\\top} \\nabla^2 f(x)\\, v\\right]\n",
    "$$\n",
    "\n",
    "Which leads to a random algorithm by sampling $M$ vectors\n",
    "$v_1,\\ldots,v_M$ and using the Monte Carlo approximation of the\n",
    "expectation, called the [Hutchinson Trace\n",
    "Estimator](https://www.tandfonline.com/doi/abs/10.1080/03610918908812806)\n",
    "\n",
    "$$\n",
    "\\mathrm{Tr}(\\nabla^2 f(x)) \\approx \\frac{1}{M} \\sum_{m=1}^M v_m^{\\top} \\nabla^2 f(x)\\, v_m\n",
    "$$\n",
    "\n",
    "### Question 1.1\n",
    "\n",
    "Now, let’s take the function $f(x) = \\frac{1}{2}x^{\\top} P x$, which is\n",
    "a quadratic form and where we know that $\\nabla^2 f(x) = P$.\n",
    "\n",
    "The following code finds the trace of the Hessian, which is equivalently\n",
    "just the sum of the diagonal of $P$ in this simple function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "10240.816\n",
      "10240.817"
     ]
    }
   ],
   "source": [
    "key = jax.random.PRNGKey(0)\n",
    "\n",
    "N = 100  # Dimension of the matrix\n",
    "A = jax.random.normal(key, (N, N))\n",
    "# Create a positive-definite matrix P by forming A^T * A\n",
    "P = jnp.dot(A.T, A)\n",
    "def f(x):\n",
    "    return 0.5 * jnp.dot(x.T, jnp.dot(P, x))\n",
    "x = jax.random.normal(key, (N,))\n",
    "print(jnp.trace(jax.hessian(f)(x)))\n",
    "print(jnp.diag(P).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, instead of calculating the whole Hessian, use a [Hessian-vector\n",
    "product in\n",
    "JAX](https://jax.readthedocs.io/en/latest/notebooks/autodiff_cookbook.html#hessian-vector-products-using-both-forward-and-reverse-mode)\n",
    "and the approximation above with $M$ draws of random vectors to\n",
    "calculate an approximation of the trace of the Hessian. Increase the\n",
    "numbers of $M$ to see what the variance of the estimator is, comparing\n",
    "to the above closed-form solution for this quadratic.\n",
    "\n",
    "Hint: you will want to do Forward-over-Reverse mode differentiation for\n",
    "this (i.e. the `vjp` gives a pullback function for first derivative,\n",
    "then differentiate that new function. Given that it would then be\n",
    "$\\mathbb{R}^N \\to \\mathbb{R}^N$, it makes sense to use forward mode with\n",
    "a `jvp`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.2 (Bonus)\n",
    "\n",
    "If you wish, you can play around with radically increase the size of the\n",
    "`N` and change the function itself. One suggestion is to move towards a\n",
    "sparse or even matrix-free $f(x)$ calculation so that the $P$ doesn’t\n",
    "itself need to materialize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "\n",
    "This section gives some hints on how to setup a differentiable\n",
    "likelihood function with implicit functions\n",
    "\n",
    "### Question 2.1\n",
    "\n",
    "The following code uses scipy to find the equilibrium price and demand\n",
    "for some simple supply and demand functions with embedded parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Equilibrium Price: 17.65\n",
      "Equilibrium Quantity: 91.60"
     ]
    }
   ],
   "source": [
    "from scipy.optimize import root_scalar\n",
    "\n",
    "# Define the demand function with power c\n",
    "def demand(P, c_d):\n",
    "    return 100 - 2 * P**c_d\n",
    "\n",
    "# Define the supply function with power f\n",
    "def supply(P, c_s):\n",
    "    return 5 * 3**(c_s * P)\n",
    "\n",
    "# Define the function to find the root of, including c and f\n",
    "def equilibrium(P, c_d, c_s):\n",
    "    return demand(P, c_d) - supply(P, c_s)\n",
    "\n",
    "# Use root_scalar to find the equilibrium price\n",
    "def find_equilibrium(c_d, c_s):\n",
    "    result = root_scalar(equilibrium, args=(c_d, c_s), bracket=[0, 100], method='brentq')\n",
    "    return result.root, demand(result.root, c_d)\n",
    "\n",
    "# Example usage\n",
    "c_d = 0.5\n",
    "c_s = 0.15\n",
    "equilibrium_price, equilibrium_quantity = find_equilibrium(c_d, c_s)\n",
    "print(f\"Equilibrium Price: {equilibrium_price:.2f}\")\n",
    "print(f\"Equilibrium Quantity: {equilibrium_quantity:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, convert this to use JAX and\n",
    "[Optimistix](https://docs.kidger.site/optimistix/) for finding the root\n",
    "using `optimistix.root_find()`. Make sure you can jit the whole\n",
    "`find_equilibrium` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.2\n",
    "\n",
    "Now, assume that you get a noisy signal on the price that fulfills that\n",
    "demand system.\n",
    "\n",
    "$$\n",
    "\\hat{p} \\sim \\mathcal{N}(p, \\sigma^2)\n",
    "$$\n",
    "\n",
    "In that case, the log likelihood for the Gaussian is\n",
    "\n",
    "$$\n",
    "\\log \\mathcal{L}(\\hat{p}\\,|\\,c_d, c_s, p) = -\\frac{1}{2} \\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2} (\\hat{p} - p)^2\n",
    "$$\n",
    "\n",
    "Or, if $p$ was implicitly defined by the equilibrium conditions as some\n",
    "$p(c_d, c_s)$ from above,\n",
    "\n",
    "$$\n",
    "\\log \\mathcal{L}(\\hat{p}\\,|\\,c_d, c_s) = -\\frac{1}{2} \\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2} (\\hat{p} - p(c_d, c_s))^2\n",
    "$$\n",
    "\n",
    "Then for some $\\sigma = 0.01$ we can calculate this log likelihood the\n",
    "above as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "metadata": {},
     "data": {
      "text/plain": [
       "np.float64(3.660406111557236)"
      ]
     }
    }
   ],
   "source": [
    "def log_likelihood(p_hat, c_d, c_s, sigma):\n",
    "    p, x = find_equilibrium(c_d, c_s)\n",
    "    return -0.5 * np.log(2 * np.pi * sigma**2) - 0.5 * (p_hat - p)**2 / sigma**2\n",
    "\n",
    "c_d = 0.5\n",
    "c_s = 0.15\n",
    "sigma = 0.01\n",
    "p, x = find_equilibrium(c_d, c_s) # get the true value for simulation\n",
    "p_hat = p + np.random.normal(0, sigma) # simulate a noisy signal\n",
    "log_likelihood(p_hat, c_d, c_s, sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, take this code for the likelihood and convert it to JAX and jit.\n",
    "Use your function from Question 2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.3\n",
    "\n",
    "Use the function from the previous part and calculate the gradient with\n",
    "respect to `params` (i.e., `c_d` and `c_s`) using `grad` and JAX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.4 (Bonus)\n",
    "\n",
    "You could try to run maximum likelihood estimation by using a\n",
    "gradient-based optimizer. You can use either\n",
    "[Optax](https://optax.readthedocs.io/) (standard for ML optimization) or\n",
    "[Optimistix](https://docs.kidger.site/optimistix/) with\n",
    "`optimistix.minimise()`.\n",
    "\n",
    "If you attempt this:\n",
    "\n",
    "-   Consider starting your optimization at the “pseudo-true” values with\n",
    "    the `c_s, c_d, sigma` you used to simulate the data and even start\n",
    "    with `p_hat = p`.\n",
    "-   You may find that it is a little too noisy with only the one\n",
    "    observation. If so, you could adapt your likelihood to take a vector\n",
    "    of $\\hat{p}$ instead. The likelihood of IID gaussians is a simple\n",
    "    variation on the above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD CODE HERE"
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "path": "/home/runner/work/grad_econ_ML/grad_econ_ML/.venv/share/jupyter/kernels/python3"
  },
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": "3"
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 }
}
